# Pipeline HoÃ n Chá»‰nh - Tá»•ng Há»£p

## âœ… Tráº¡ng ThÃ¡i Hiá»‡n Táº¡i

### 1. STEP 4: Contrastive Context Scoring - âœ… HOÃ€N THÃ€NH
**File**: `phrase_centric_extractor.py`

**Chá»©c nÄƒng**:
- PhÃ¢n tÃ­ch ngá»¯ cáº£nh xuáº¥t hiá»‡n cá»§a má»—i cá»¥m tá»«
- PhÃ¢n loáº¡i cÃ¢u thÃ nh:
  - **Positive context**: MÃ´ táº£, thÃ´ng tin, khÃ¡ch quan
  - **Negative context**: Discourse markers, template, Ã½ kiáº¿n

**CÃ´ng thá»©c**:
```
contrastive_score = (N_positive - N_negative) / (N_positive + N_negative)
```

**Discourse Markers** (negative context):
- Opinion: "in my opinion", "i think", "i believe"
- Vague: "many people think", "some people say"
- Temporal: "nowadays", "these days", "in modern times"
- Discourse: "in conclusion", "to sum up", "on the one hand"
- Template: "it is clear that", "obviously", "there are many"

**Output**:
- `contrastive_score`: -1.0 Ä‘áº¿n 1.0
- `positive_contexts`: Sá»‘ láº§n xuáº¥t hiá»‡n trong positive context
- `negative_contexts`: Sá»‘ láº§n xuáº¥t hiá»‡n trong negative context

---

### 2. Single-Word Extraction - âœ… ÄÃƒ CÃ“ Sáº´N
**File**: `single_word_extractor.py`

**Chá»©c nÄƒng**:
- TrÃ­ch xuáº¥t tá»« Ä‘Æ¡n cÃ³ giÃ¡ trá»‹ há»c táº­p
- Bá»• sung cho phrases (khÃ´ng cáº¡nh tranh)
- Soft filtering approach (khÃ´ng hard drop)

**Pipeline**:
```
STEP 7.1: POS Constraint (NOUN, VERB, ADJ only) âœ…
STEP 7.2: Stopword & Function-word Removal âœ…
STEP 7.3: Calculate Rarity Penalty (SOFT) âœ…
STEP 7.4: Calculate Learning Value Score (CORE) âœ…
STEP 7.5: Calculate Phrase Coverage Penalty (SOFT) âœ…
STEP 7.6: Heading-aware Semantic Filter âœ…
STEP 7.7: Lexical Specificity Check âœ…
STEP 7.8: Final Scoring & Ranking âœ…
```

**Learning Value Formula**:
```python
learning_value = (
    concreteness * 0.3 +
    domain_specificity * 0.3 +
    morphological_richness * 0.2 -
    generality_penalty * 0.2
)
```

**Final Score Formula**:
```python
final_score = learning_value - (
    rarity_penalty * 0.2 +
    coverage_penalty * 0.5
)
```

**VÃ­ dá»¥**:
- "mitigation": learning_value=0.86, penalties=0.02 â†’ final_score=0.84 âœ… KEEP
- "important": learning_value=0.24, penalties=0.12 â†’ final_score=0.12 âŒ DROP
- "learning" (trong phrase): learning_value=0.53, penalties=0.45 â†’ final_score=0.08 âŒ DROP

---

## ğŸ“Š Kiáº¿n TrÃºc Tá»•ng Thá»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT INPUT                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PHRASE EXTRACTION PIPELINE                      â”‚
â”‚  (phrase_centric_extractor.py)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 1: Sentence-Level Analysis                            â”‚
â”‚  STEP 2: Candidate Phrase Extraction                        â”‚
â”‚  STEP 3: Hard Filtering Rules                               â”‚
â”‚  STEP 3.1: POS Structure Filter                             â”‚
â”‚  STEP 3.2: Lexical Specificity Filter (SOFT)                â”‚
â”‚  STEP 3B: Statistical + Semantic Refinement                 â”‚
â”‚    - TF-IDF Scoring                                          â”‚
â”‚    - SBERT Embeddings                                        â”‚
â”‚    - K-Means Clustering (Elbow Method)                      â”‚
â”‚    - Cluster Representatives Selection                       â”‚
â”‚  STEP 3.3: Phrase Rarity Filter - SKIPPED                   â”‚
â”‚  STEP 4: Contrastive Context Scoring âœ… NEW                 â”‚
â”‚  STEP 5-8: SKIPPED                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    PHRASES OUTPUT
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SINGLE-WORD EXTRACTION PIPELINE                    â”‚
â”‚  (single_word_extractor.py)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  STEP 7.1: POS Constraint (NOUN, VERB, ADJ)                 â”‚
â”‚  STEP 7.2: Stopword Removal                                 â”‚
â”‚  STEP 7.3: Rarity Penalty (SOFT)                            â”‚
â”‚  STEP 7.4: Learning Value Score (CORE)                      â”‚
â”‚  STEP 7.5: Phrase Coverage Penalty (SOFT)                   â”‚
â”‚  STEP 7.6: Semantic Filter                                  â”‚
â”‚  STEP 7.7: Lexical Specificity Check                        â”‚
â”‚  STEP 7.8: Final Scoring & Ranking                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   SINGLE WORDS OUTPUT
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MERGE & DEDUPLICATE                       â”‚
â”‚  (phrase_word_merger.py)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  FINAL VOCABULARY LIST
```

---

## ğŸ”§ CÃ¡ch Sá»­ Dá»¥ng

### 1. TrÃ­ch Xuáº¥t Phrases

```python
from phrase_centric_extractor import PhraseCentricExtractor

extractor = PhraseCentricExtractor()

phrases = extractor.extract_vocabulary(
    text=document_text,
    document_title="Climate Change",
    max_phrases=50
)

# Output: List of phrase dictionaries with:
# - phrase: str
# - frequency: int
# - contrastive_score: float
# - importance_score: float
# - supporting_sentence: str
```

### 2. TrÃ­ch Xuáº¥t Single Words

```python
from single_word_extractor import SingleWordExtractor

word_extractor = SingleWordExtractor()

single_words = word_extractor.extract_single_words(
    text=document_text,
    phrases=phrases,  # From step 1
    headings=headings,
    max_words=20
)

# Output: List of word dictionaries with:
# - word: str
# - frequency: int
# - learning_value: float
# - final_score: float
# - supporting_sentence: str
```

### 3. Merge Results

```python
from phrase_word_merger import merge_vocabulary

final_vocabulary = merge_vocabulary(
    phrases=phrases,
    single_words=single_words
)
```

---

## ğŸ“ˆ VÃ­ Dá»¥ Output

### Phrases (tá»« STEP 4):
```
1. 'climate change' (contrastive: 0.857, freq: 6, score: 0.95)
2. 'greenhouse gases' (contrastive: 1.000, freq: 3, score: 0.85)
3. 'fossil fuels' (contrastive: 0.750, freq: 4, score: 0.82)
```

### Single Words (tá»« STEP 7):
```
1. 'mitigation' (learning_value: 0.86, final_score: 0.84)
   - Concreteness: 0.80
   - Domain Specificity: 0.85
   - Morphological Richness: 0.90
   - Generality Penalty: 0.00
   - Rarity Penalty: 0.10
   - Coverage Penalty: 0.00

2. 'photosynthesis' (learning_value: 0.92, final_score: 0.88)
   - Concreteness: 1.00
   - Domain Specificity: 0.90
   - Morphological Richness: 0.90
   - Generality Penalty: 0.00
   - Rarity Penalty: 0.05
   - Coverage Penalty: 0.00
```

---

## ğŸ¯ Äiá»ƒm Máº¡nh

### Phrase Extraction:
1. âœ… Contrastive Context Scoring - phÃ¢n biá»‡t content vs discourse
2. âœ… SOFT downgrade cho umbrella concepts
3. âœ… Statistical + Semantic refinement (TF-IDF, SBERT, K-Means)
4. âœ… Cluster-based representative selection

### Single-Word Extraction:
1. âœ… Learning Value Score - Ä‘Ã¡nh giÃ¡ giÃ¡ trá»‹ há»c táº­p
2. âœ… Soft filtering - khÃ´ng hard drop
3. âœ… Phrase coverage penalty - trÃ¡nh trÃ¹ng láº·p
4. âœ… Multi-dimensional scoring (concreteness, domain, morphology)

---

## ğŸš€ CÃ¡c BÆ°á»›c Tiáº¿p Theo

### ÄÃ£ HoÃ n ThÃ nh:
- [x] STEP 4: Contrastive Context Scoring
- [x] Single-Word Extraction module
- [x] Soft filtering approach
- [x] Learning value calculation

### Cáº§n Kiá»ƒm Tra:
- [ ] Fix syntax errors trong `phrase_centric_extractor.py`
- [ ] Test STEP 4 vá»›i real documents
- [ ] Test single-word extraction
- [ ] Verify merge logic trong `phrase_word_merger.py`
- [ ] Integration test vá»›i complete pipeline

### Tá»‘i Æ¯u HÃ³a (Optional):
- [ ] Fine-tune thresholds (IDF, semantic similarity)
- [ ] Optimize learning value weights
- [ ] Add more discourse markers
- [ ] Improve morphological analysis

---

## ğŸ“ Ghi ChÃº Quan Trá»ng

1. **STEP 4 Ä‘Ã£ Ä‘Æ°á»£c implement** theo Ä‘Ãºng specification cá»§a báº¡n
2. **Single-word extractor Ä‘Ã£ cÃ³ sáºµn** vá»›i soft filtering approach
3. **Váº«n cÃ²n syntax error** trong `phrase_centric_extractor.py` cáº§n fix
4. **Pipeline hoÃ n chá»‰nh** khi cáº£ 2 modules hoáº¡t Ä‘á»™ng tá»‘t

---

## ğŸ” Debugging

Náº¿u gáº·p lá»—i, kiá»ƒm tra:
1. Python cache Ä‘Ã£ clear chÆ°a: `rm -rf __pycache__`
2. Dependencies Ä‘Ã£ cÃ i Ä‘á»§ chÆ°a: `pip install -r requirements.txt`
3. spaCy model Ä‘Ã£ download chÆ°a: `python -m spacy download en_core_web_sm`
4. sentence-transformers Ä‘Ã£ cÃ i chÆ°a: `pip install sentence-transformers`

---

**Status**: âœ… STEP 4 DONE | âœ… SINGLE-WORD MODULE READY | âš ï¸ SYNTAX FIX NEEDED
