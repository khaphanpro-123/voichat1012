"""
Document Embedding System
Táº¡o semantic embeddings cho documents vÃ  há»— trá»£ semantic search

Cháº¡y SONG SONG vá»›i TF-IDF pipeline, khÃ´ng thay tháº¿
"""

import numpy as np
from typing import List, Dict, Tuple
from sklearn.metrics.pairwise import cosine_similarity

# Try to import sentence-transformers
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸  sentence-transformers not installed. Run: pip install sentence-transformers")


# ============================================================================
# EMBEDDING CREATION
# ============================================================================

class DocumentEmbedder:
    """
    Táº¡o embeddings cho documents sá»­ dá»¥ng Sentence-BERT
    
    Model: all-MiniLM-L6-v2
    - KÃ­ch thÆ°á»›c: 384 dimensions
    - Nhanh vÃ  hiá»‡u quáº£
    - PhÃ¹ há»£p cho semantic search
    """
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        Initialize embedder
        
        Args:
            model_name: TÃªn model Sentence-BERT
        """
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "sentence-transformers not installed. "
                "Run: pip install sentence-transformers"
            )
        
        print(f"[Embedding] Loading model: {model_name}...")
        self.model = SentenceTransformer(model_name)
        self.model_name = model_name
        print(f"[Embedding] Model loaded successfully")
    
    def encode_documents(
        self,
        documents: List[str],
        show_progress: bool = True
    ) -> np.ndarray:
        """
        Táº¡o embeddings cho danh sÃ¡ch documents
        
        Args:
            documents: Danh sÃ¡ch vÄƒn báº£n
            show_progress: Hiá»ƒn thá»‹ progress bar
        
        Returns:
            numpy array shape (n_documents, embedding_dim)
        """
        print(f"[Embedding] Encoding {len(documents)} documents...")
        
        embeddings = self.model.encode(
            documents,
            show_progress_bar=show_progress,
            convert_to_numpy=True
        )
        
        print(f"[Embedding] Created embeddings: {embeddings.shape}")
        return embeddings
    
    def encode_query(self, query: str) -> np.ndarray:
        """
        Táº¡o embedding cho query
        
        Args:
            query: CÃ¢u query
        
        Returns:
            numpy array shape (1, embedding_dim)
        """
        embedding = self.model.encode([query], convert_to_numpy=True)
        return embedding


# ============================================================================
# SEMANTIC SEARCH
# ============================================================================

def semantic_search(
    query_embedding: np.ndarray,
    document_embeddings: np.ndarray,
    documents: List[str],
    top_k: int = 5,
    threshold: float = 0.0
) -> List[Dict]:
    """
    TÃ¬m kiáº¿m documents dá»±a trÃªn semantic similarity
    
    Args:
        query_embedding: Embedding cá»§a query (1, dim)
        document_embeddings: Embeddings cá»§a documents (n, dim)
        documents: Danh sÃ¡ch vÄƒn báº£n gá»‘c
        top_k: Sá»‘ káº¿t quáº£ tráº£ vá»
        threshold: NgÆ°á»¡ng similarity tá»‘i thiá»ƒu
    
    Returns:
        List of {document, similarity, rank}
    """
    # Calculate cosine similarity
    similarities = cosine_similarity(query_embedding, document_embeddings)[0]
    
    # Get top-k indices
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    # Build results
    results = []
    for rank, idx in enumerate(top_indices):
        similarity = float(similarities[idx])
        
        # Filter by threshold
        if similarity >= threshold:
            results.append({
                'document': documents[idx],
                'document_id': int(idx),
                'similarity': similarity,
                'rank': rank + 1
            })
    
    return results


def find_similar_documents(
    document_id: int,
    document_embeddings: np.ndarray,
    documents: List[str],
    top_k: int = 5,
    exclude_self: bool = True
) -> List[Dict]:
    """
    TÃ¬m documents tÆ°Æ¡ng tá»± vá»›i document cho trÆ°á»›c
    
    Args:
        document_id: ID cá»§a document cáº§n tÃ¬m similar
        document_embeddings: Embeddings cá»§a táº¥t cáº£ documents
        documents: Danh sÃ¡ch vÄƒn báº£n gá»‘c
        top_k: Sá»‘ káº¿t quáº£ tráº£ vá»
        exclude_self: Loáº¡i bá» chÃ­nh document Ä‘Ã³
    
    Returns:
        List of {document, similarity, rank}
    """
    # Get embedding of target document
    target_embedding = document_embeddings[document_id:document_id+1]
    
    # Calculate similarities
    similarities = cosine_similarity(target_embedding, document_embeddings)[0]
    
    # Exclude self if needed
    if exclude_self:
        similarities[document_id] = -1
    
    # Get top-k
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    results = []
    for rank, idx in enumerate(top_indices):
        if idx == document_id and exclude_self:
            continue
        
        results.append({
            'document': documents[idx],
            'document_id': int(idx),
            'similarity': float(similarities[idx]),
            'rank': rank + 1
        })
    
    return results


# ============================================================================
# DOCUMENT CLUSTERING USING EMBEDDINGS (OPTIONAL)
# ============================================================================

def cluster_documents_by_embedding(
    embeddings: np.ndarray,
    n_clusters: int,
    documents: List[str]
) -> Dict:
    """
    PhÃ¢n cá»¥m documents dá»±a trÃªn embeddings
    
    LÆ¯U Ã: ÄÃ¢y lÃ  phÆ°Æ¡ng phÃ¡p THAY THáº¾ cho TF-IDF clustering
    Chá»‰ dÃ¹ng khi muá»‘n clustering dá»±a trÃªn ngá»¯ nghÄ©a sÃ¢u
    
    Args:
        embeddings: Document embeddings
        n_clusters: Sá»‘ clusters
        documents: Danh sÃ¡ch vÄƒn báº£n
    
    Returns:
        Dictionary vá»›i cluster assignments
    """
    from sklearn.cluster import KMeans
    
    print(f"[Embedding Clustering] Clustering {len(documents)} documents into {n_clusters} clusters...")
    
    # K-means on embeddings
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(embeddings)
    
    # Organize by cluster
    clusters = {}
    for idx, label in enumerate(cluster_labels):
        if label not in clusters:
            clusters[label] = []
        
        clusters[label].append({
            'document': documents[idx],
            'document_id': idx
        })
    
    return {
        'clusters': clusters,
        'labels': cluster_labels.tolist(),
        'n_clusters': n_clusters,
        'method': 'K-Means on Embeddings'
    }


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def calculate_similarity_matrix(embeddings: np.ndarray) -> np.ndarray:
    """
    TÃ­nh ma tráº­n similarity giá»¯a táº¥t cáº£ documents
    
    Args:
        embeddings: Document embeddings (n, dim)
    
    Returns:
        Similarity matrix (n, n)
    """
    return cosine_similarity(embeddings)


def get_embedding_statistics(embeddings: np.ndarray) -> Dict:
    """
    Thá»‘ng kÃª vá» embeddings
    
    Args:
        embeddings: Document embeddings
    
    Returns:
        Dictionary vá»›i statistics
    """
    return {
        'n_documents': embeddings.shape[0],
        'embedding_dim': embeddings.shape[1],
        'mean_norm': float(np.linalg.norm(embeddings, axis=1).mean()),
        'std_norm': float(np.linalg.norm(embeddings, axis=1).std())
    }


# ============================================================================
# TESTING
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("TESTING DOCUMENT EMBEDDING SYSTEM")
    print("=" * 80)
    
    # Test documents
    documents = [
        "Machine learning is a subset of artificial intelligence",
        "Deep learning uses neural networks with multiple layers",
        "Natural language processing helps computers understand text",
        "Football is a popular sport played worldwide",
        "Basketball players need good coordination and teamwork"
    ]
    
    # Create embedder
    try:
        embedder = DocumentEmbedder()
        
        # Encode documents
        doc_embeddings = embedder.encode_documents(documents, show_progress=False)
        print(f"\nâœ… Document embeddings created: {doc_embeddings.shape}")
        
        # Test semantic search
        query = "AI and machine learning applications"
        query_embedding = embedder.encode_query(query)
        
        print(f"\nğŸ” Searching for: '{query}'")
        results = semantic_search(
            query_embedding,
            doc_embeddings,
            documents,
            top_k=3
        )
        
        print("\nğŸ“Š Search Results:")
        for result in results:
            print(f"  Rank {result['rank']}: {result['document'][:60]}...")
            print(f"    Similarity: {result['similarity']:.4f}")
        
        # Test similar documents
        print(f"\nğŸ”— Finding documents similar to document 0...")
        similar = find_similar_documents(
            0,
            doc_embeddings,
            documents,
            top_k=3
        )
        
        print("\nğŸ“Š Similar Documents:")
        for result in similar:
            print(f"  Rank {result['rank']}: {result['document'][:60]}...")
            print(f"    Similarity: {result['similarity']:.4f}")
        
        # Statistics
        stats = get_embedding_statistics(doc_embeddings)
        print(f"\nğŸ“ˆ Embedding Statistics:")
        print(f"  Documents: {stats['n_documents']}")
        print(f"  Dimensions: {stats['embedding_dim']}")
        print(f"  Mean norm: {stats['mean_norm']:.4f}")
        
        print("\nâœ… All tests passed!")
        
    except ImportError as e:
        print(f"\nâŒ Error: {e}")
        print("Install sentence-transformers: pip install sentence-transformers")
