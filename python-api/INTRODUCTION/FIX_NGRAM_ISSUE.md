# ğŸ”§ FIX: Váº¥n Ä‘á» chá»‰ trÃ­ch xuáº¥t tá»« Ä‘Æ¡n

## âŒ Váº¤N Äá»€

1. **Chá»‰ cÃ³ 10 flashcards** - Giá»›i háº¡n máº·c Ä‘á»‹nh `max_cards=10`
2. **ToÃ n tá»« Ä‘Æ¡n (unigrams)** - Máº¥t háº¿t bigrams/phrases nhÆ° "machine learning", "deep learning"

## ğŸ” NGUYÃŠN NHÃ‚N

### Váº¥n Ä‘á» 1: Filter bigrams quÃ¡ nghiÃªm

**File**: `ensemble_extractor.py` (line ~370)

```python
# Bigrams: keep if both words are meaningful
elif len(words) == 2:
    if all(len(w) >= 3 and w not in ENGLISH_STOPWORDS for w in words):
        filtered_candidates.add(c)
```

**Váº¥n Ä‘á»**: 
- YÃªu cáº§u cáº£ 2 tá»« pháº£i >= 3 kÃ½ tá»±
- Loáº¡i bá» stopwords â†’ máº¥t phrases nhÆ° "in healthcare", "of learning"

### Váº¥n Ä‘á» 2: TF-IDF config

**File**: `ensemble_extractor.py` (line ~130)

```python
vectorizer = TfidfVectorizer(
    max_features=1000,
    ngram_range=(1, 2),      # OK
    min_df=2,                # QuÃ¡ cao cho bigrams
    max_df=0.8,              # OK
    stop_words='english',    # Loáº¡i bá» bigrams cÃ³ stopwords
    norm='l2'
)
```

**Váº¥n Ä‘á»**:
- `min_df=2`: Bigrams xuáº¥t hiá»‡n < 2 láº§n bá»‹ loáº¡i
- `stop_words='english'`: Loáº¡i cáº£ bigrams chá»©a stopwords

### Váº¥n Ä‘á» 3: Flashcard limit

**File**: `main.py` (line ~650)

```python
flashcards_result = rag_system.generate_flashcards(
    document_id=document_id,
    max_cards=min(10, len(vocabulary_contexts))  # Chá»‰ 10!
)
```

---

## âœ… GIáº¢I PHÃP

### Fix 1: Ná»›i lá»ng filter bigrams

**Thay Ä‘á»•i trong `ensemble_extractor.py`:**

```python
# Bigrams: keep if meaningful (ná»›i lá»ng hÆ¡n)
elif len(words) == 2:
    # Chá»‰ cáº§n 1 trong 2 tá»« cÃ³ nghÄ©a
    meaningful_words = [w for w in words if len(w) >= 3 and w not in ENGLISH_STOPWORDS]
    if len(meaningful_words) >= 1:  # Thay vÃ¬ all()
        filtered_candidates.add(c)
```

### Fix 2: Táº¡o TF-IDF riÃªng cho n-grams

**ThÃªm vÃ o `ensemble_extractor.py`:**

```python
def calculate_tfidf_ngrams(documents: List[str]) -> Dict[str, float]:
    """TF-IDF riÃªng cho n-grams, khÃ´ng filter stopwords"""
    vectorizer = TfidfVectorizer(
        max_features=500,
        ngram_range=(2, 3),      # Chá»‰ bigrams vÃ  trigrams
        min_df=1,                # Giáº£m xuá»‘ng 1
        max_df=0.9,              # Ná»›i lá»ng
        # KHÃ”NG dÃ¹ng stop_words Ä‘á»ƒ giá»¯ phrases
        norm='l2'
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(documents)
        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix[0].toarray()[0]
        
        tfidf_scores = {}
        for idx, score in enumerate(scores):
            if score > 0:
                tfidf_scores[feature_names[idx]] = score
        
        return tfidf_scores
    except:
        return {}
```

### Fix 3: TÄƒng sá»‘ flashcards

**Thay Ä‘á»•i trong `main.py`:**

```python
# TÄƒng tá»« 10 lÃªn 20 hoáº·c táº¥t cáº£
flashcards_result = rag_system.generate_flashcards(
    document_id=document_id,
    max_cards=min(20, len(vocabulary_contexts))  # TÄƒng lÃªn 20
)
```

Hoáº·c cho phÃ©p user chá»n:

```python
max_cards = request.max_cards if hasattr(request, 'max_cards') else 20
```

---

## ğŸš€ IMPLEMENT NGAY

TÃ´i sáº½ implement 3 fixes nÃ y cho báº¡n!

---

## ğŸ“Š Káº¾T QUáº¢ MONG Äá»¢I

### TrÆ°á»›c fix:
```json
{
  "vocabulary": [
    {"word": "machine"},
    {"word": "learning"},
    {"word": "deep"},
    {"word": "healthcare"}
  ]
}
```

### Sau fix:
```json
{
  "vocabulary": [
    {"word": "machine learning"},
    {"word": "deep learning"},
    {"word": "healthcare system"},
    {"word": "medical image"},
    {"word": "diagnosis accuracy"}
  ]
}
```

---

## ğŸ¯ PRIORITY

1. **HIGH**: Fix bigram filter (ngay láº­p tá»©c)
2. **HIGH**: TÄƒng flashcard limit
3. **MEDIUM**: TF-IDF riÃªng cho n-grams

Báº¡n muá»‘n tÃ´i implement ngay khÃ´ng? ğŸš€
