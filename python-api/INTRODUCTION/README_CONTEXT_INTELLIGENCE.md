# Context Intelligence Engine - Documentation

## ğŸ“– Tá»•ng quan

Context Intelligence Engine lÃ  há»‡ thá»‘ng chá»n cÃ¢u ngá»¯ cáº£nh tá»‘t nháº¥t cho má»—i tá»« vá»±ng, Ä‘Æ°á»£c implement báº±ng Python vá»›i cÃ¡c thÆ° viá»‡n NLP chuyÃªn nghiá»‡p.

## ğŸ¯ Má»¥c tiÃªu

Cho má»—i tá»« vá»±ng Ä‘Æ°á»£c trÃ­ch xuáº¥t, há»‡ thá»‘ng sáº½:
1. TÃ¬m táº¥t cáº£ cÃ¢u chá»©a tá»« Ä‘Ã³
2. Lá»c bá» cÃ¢u khÃ´ng há»£p lá»‡
3. Cháº¥m Ä‘iá»ƒm cÃ¡c cÃ¢u cÃ²n láº¡i
4. Chá»n cÃ¢u tá»‘t nháº¥t
5. Highlight tá»« vá»±ng trong cÃ¢u

## ğŸ”§ Pipeline

### STAGE 1: Ensemble Vocabulary Extraction
```
Input: Raw text
â†“
Clean metadata â†’ Tokenize â†’ Extract candidates
â†“
Calculate features: TF-IDF, RAKE, YAKE, Frequency
â†“
Normalize & Weight â†’ Filter â†’ Rank
â†“
Output: Top N vocabulary words with scores
```

### STAGE 2: Context Intelligence Engine
```
Input: Text + Vocabulary list
â†“
Build Sentence objects (with metadata)
â†“
Map words â†’ sentences
â†“
Filter invalid sentences
â†“
Score sentences (multi-criteria)
â†“
Select best sentence per word
â†“
Highlight word in sentence
â†“
Output: Vocabulary with context
```

## ğŸ“Š Scoring Formula

### Sentence Score
```python
score = 0.4 Ã— keyword_density +
        0.3 Ã— length_score +
        0.2 Ã— position_score +
        0.1 Ã— clarity_score
```

### Components

#### 1. Keyword Density (40%)
```python
keyword_density = count(vocabulary_words_in_sentence) / total_words
```
- Äo lÆ°á»ng máº­t Ä‘á»™ tá»« khÃ³a quan trá»ng
- Cao hÆ¡n = nhiá»u tá»« vá»±ng hÆ¡n trong cÃ¢u

#### 2. Length Score (30%)
```python
if 8 â‰¤ word_count â‰¤ 20:
    length_score = 1.0  # Perfect
elif word_count < 8:
    length_score = word_count / 8  # Linear penalty
else:
    length_score = exp(-(word_count - 20) / 10)  # Exponential penalty
```
- CÃ¢u 8-20 tá»« lÃ  lÃ½ tÆ°á»Ÿng
- QuÃ¡ ngáº¯n hoáº·c quÃ¡ dÃ i bá»‹ pháº¡t Ä‘iá»ƒm

#### 3. Position Score (20%)
```python
position_score = exp(-position / (total_sentences Ã— 0.3))
```
- CÃ¢u xuáº¥t hiá»‡n sá»›m hÆ¡n â†’ quan trá»ng hÆ¡n
- Exponential decay

#### 4. Clarity Score (10%)
```python
score = 0
if has_verb: score += 0.5
if comma_ratio < 0.15: score += 0.3
if not starts_with_bullet: score += 0.2
```
- CÃ³ Ä‘á»™ng tá»«
- KhÃ´ng pháº£i list
- CÃ¢u tá»± nhiÃªn

## ğŸ› ï¸ ThÆ° viá»‡n sá»­ dá»¥ng

### spaCy
```python
import spacy
nlp = spacy.load("en_core_web_sm")

# POS tagging
doc = nlp(text)
for token in doc:
    print(token.text, token.pos_)

# Verb detection
has_verb = any(token.pos_ == "VERB" for token in doc)
```

### NLTK
```python
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

# Sentence tokenization
sentences = sent_tokenize(text)

# Word tokenization
words = word_tokenize(text)

# Stopwords
stop_words = set(stopwords.words('english'))
```

### scikit-learn
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF
vectorizer = TfidfVectorizer(ngram_range=(1, 3))
tfidf_matrix = vectorizer.fit_transform([text])
```

## ğŸ“ VÃ­ dá»¥ sá»­ dá»¥ng

### Python Code
```python
from ensemble_extractor import extract_vocabulary_ensemble
from context_intelligence import select_vocabulary_contexts

# STAGE 1: Extract vocabulary
text = "Your document text here..."
ensemble_result = extract_vocabulary_ensemble(
    text,
    max_words=50,
    weights={
        'frequency': 0.15,
        'tfidf': 0.35,
        'rake': 0.25,
        'yake': 0.25
    }
)

# Prepare vocabulary list
vocabulary_list = [
    {'word': score['word'], 'score': score['score']}
    for score in ensemble_result['scores']
]

# STAGE 2: Select contexts
contexts = select_vocabulary_contexts(
    text,
    vocabulary_list,
    language="en",
    min_words=5,
    max_words=35,
    require_verb=True
)

# Print results
for ctx in contexts:
    print(f"Word: {ctx['word']}")
    print(f"Context: {ctx['contextSentence']}")
    print(f"Score: {ctx['sentenceScore']:.3f}")
    print(f"Explanation: {ctx['explanation']}")
```

### API Call
```bash
curl -X POST http://localhost:8000/api/smart-vocabulary-extract \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Machine learning is a subset of artificial intelligence...",
    "max_words": 20,
    "language": "en"
  }'
```

### Response
```json
{
  "success": true,
  "vocabulary": [
    {
      "word": "machine learning",
      "score": 0.856,
      "context": "<b>Machine learning</b> is a subset of artificial intelligence.",
      "contextPlain": "Machine learning is a subset of artificial intelligence.",
      "sentenceId": "s1",
      "sentenceScore": 0.823,
      "explanation": "ÄÆ°á»£c chá»n vÃ¬: máº­t Ä‘á»™ tá»« khÃ³a cao (15.2%), Ä‘á»™ dÃ i tá»‘i Æ°u (9 tá»«). Score: 0.823",
      "features": {
        "frequency": 0.045,
        "tfidf": 0.678,
        "rake": 4.5,
        "yake": 0.234
      }
    }
  ],
  "count": 15,
  "stats": {...}
}
```

## âœ… Quality Checkpoints

### 1. Explainable Scoring
Má»—i cÃ¢u Ä‘Æ°á»£c chá»n Ä‘á»u cÃ³ explanation rÃµ rÃ ng:
```
"ÄÆ°á»£c chá»n vÃ¬: máº­t Ä‘á»™ tá»« khÃ³a cao (15.2%), Ä‘á»™ dÃ i tá»‘i Æ°u (9 tá»«), 
xuáº¥t hiá»‡n sá»›m trong tÃ i liá»‡u, cÃ¢u rÃµ rÃ ng cÃ³ Ä‘á»™ng tá»«. Score: 0.823"
```

### 2. Deterministic Output
CÃ¹ng input â†’ cÃ¹ng output (khÃ´ng random)

### 3. Readable Contexts
- CÃ¢u Ä‘á»§ dÃ i (5-35 tá»«)
- CÃ³ Ä‘á»™ng tá»«
- KhÃ´ng pháº£i list hoáº·c tiÃªu Ä‘á»
- Tá»« vá»±ng Ä‘Æ°á»£c highlight rÃµ rÃ ng

## ğŸ§ª Testing

```bash
# Run all tests
python test_context_intelligence.py

# Test individual stages
python -c "from ensemble_extractor import extract_vocabulary_ensemble; ..."
python -c "from context_intelligence import select_vocabulary_contexts; ..."
```

## ğŸ“ LÃ½ thuyáº¿t

### TF-IDF (Term Frequency-Inverse Document Frequency)
```
TF(t,d) = f(t,d) / max{f(w,d) : w âˆˆ d}
IDF(t,D) = log(|D| / |{d âˆˆ D : t âˆˆ d}|)
TF-IDF(t,d,D) = TF(t,d) Ã— IDF(t,D)
```

### RAKE (Rapid Automatic Keyword Extraction)
```
RAKE_score(word) = degree(word) / frequency(word)
```
- degree = tá»•ng sá»‘ tá»« trong cÃ¡c cá»¥m chá»©a tá»« Ä‘Ã³
- frequency = sá»‘ láº§n xuáº¥t hiá»‡n

### YAKE (Yet Another Keyword Extractor)
```
YAKE_score = (relatedness Ã— position) / (case + frequency + different_sentences)
```
- Káº¿t há»£p nhiá»u features: vá»‹ trÃ­, táº§n suáº¥t, ngá»¯ cáº£nh

## ğŸ” Filtering Rules

### Invalid Sentences
- QuÃ¡ ngáº¯n: < 5 tá»«
- QuÃ¡ dÃ i: > 35 tá»«
- KhÃ´ng cÃ³ Ä‘á»™ng tá»«
- ToÃ n chá»¯ hoa (tiÃªu Ä‘á»)
- Báº¯t Ä‘áº§u báº±ng bullet point

### Unwanted Terms
- Proper nouns (tÃªn riÃªng)
- Technical metadata (pdf, doc, http, etc.)
- Pure numbers
- Very short terms (< 3 chars)

## ğŸ“ˆ Performance

| Stage | Time | Memory |
|-------|------|--------|
| STAGE 1 (1000 words) | ~2-3s | ~50MB |
| STAGE 2 (50 vocab) | ~1-2s | ~30MB |
| **Total** | **~3-5s** | **~80MB** |

## ğŸš€ Optimization Tips

1. **Batch Processing**: Process multiple documents together
2. **Caching**: Cache TF-IDF vectors for similar documents
3. **Parallel Processing**: Use multiprocessing for large documents
4. **Model Loading**: Load spaCy model once at startup

## ğŸ“š References

- [spaCy Documentation](https://spacy.io/)
- [NLTK Documentation](https://www.nltk.org/)
- [scikit-learn TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)
- [RAKE Paper](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents)
- [YAKE Paper](https://repositorio.inesctec.pt/handle/123456789/7623)

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch
3. Add tests
4. Submit pull request

## ğŸ“„ License

MIT License
