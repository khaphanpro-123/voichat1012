# Äá»€ XUáº¤T PIPELINE Xá»¬ LÃ TÃ€I LIá»†U HOÃ€N CHá»ˆNH

## I. Má»¤C TIÃŠU Há»† THá»NG (Problem & Objective)

### BÃ i toÃ¡n

NgÆ°á»i dÃ¹ng táº£i lÃªn tÃ i liá»‡u (PDF scan / áº£nh, tiáº¿ng Anh). Há»‡ thá»‘ng cáº§n:

1. **TrÃ­ch xuáº¥t ná»™i dung vÄƒn báº£n** tá»« tÃ i liá»‡u khÃ´ng cÃ³ cáº¥u trÃºc
2. **Hiá»ƒu ná»™i dung tÃ i liá»‡u** thÃ´ng qua phÃ¢n tÃ­ch ngá»¯ nghÄ©a
3. **PhÃ¢n nhÃ³m tÃ i liá»‡u theo chá»§ Ä‘á»** má»™t cÃ¡ch tá»± Ä‘á»™ng
4. **TrÃ­ch cá»¥m tá»« Ä‘áº¡i diá»‡n** cho tá»«ng nhÃ³m Ä‘á»ƒ giáº£i thÃ­ch ná»™i dung
5. **Há»— trá»£ tÃ¬m kiáº¿m theo ngá»¯ nghÄ©a** Ä‘á»ƒ ngÆ°á»i dÃ¹ng dá»… dÃ ng truy xuáº¥t thÃ´ng tin

### Má»¥c tiÃªu

Äá» xuáº¥t má»™t **pipeline xá»­ lÃ½ tÃ i liá»‡u tá»± Ä‘á»™ng** káº¿t há»£p:

- **OCR** (Optical Character Recognition)
- **TF-IDF vá»›i n-gram** (Term Frequency-Inverse Document Frequency)
- **K-means Clustering** vá»›i **Elbow Method**
- **Keyword/Phrase Extraction**
- **Embedding ngá»¯ nghÄ©a** (Semantic Embeddings)

---

## II. KIáº¾N TRÃšC & PIPELINE Äá»€ XUáº¤T (Proposed Pipeline)

### ğŸ”· Tá»•ng quan Pipeline

```
User upload document
        â†“
    OCR (BÆ¯á»šC 1)
        â†“
Text preprocessing (BÆ¯á»šC 2)
        â†“
TF-IDF (n-gram) (BÆ¯á»šC 3)
        â†“
Elbow â†’ chá»n K (BÆ¯á»šC 4)
        â†“
K-means clustering (BÆ¯á»šC 5)
        â†“
Keyword / Phrase extraction (BÆ¯á»šC 6)
(giáº£i thÃ­ch cluster)
        â†“
Embedding (BÆ¯á»šC 7)
(cháº¡y song song)
        â†“
Semantic search / similarity (BÆ¯á»šC 8)
```

---

## III. CHI TIáº¾T Tá»ªNG BÆ¯á»šC

### ğŸ”¹ BÆ¯á»šC 0 â€“ NgÆ°á»i dÃ¹ng táº£i tÃ i liá»‡u

**Input:**
- PDF scan
- áº¢nh (jpg, png)
- VÄƒn báº£n tiáº¿ng Anh

**Output:**
- File thÃ´

**ğŸ“Œ LÆ°u Ã½:**
- ChÆ°a xá»­ lÃ½ NLP á»Ÿ bÆ°á»›c nÃ y
- Chá»‰ lÆ°u trá»¯ file vÃ  chuáº©n bá»‹ cho OCR

---

### ğŸ”¹ BÆ¯á»šC 1 â€“ OCR (Optical Character Recognition)

**Má»¥c Ä‘Ã­ch:**
- Chuyá»ƒn dá»¯ liá»‡u khÃ´ng cáº¥u trÃºc (áº£nh) â†’ text

**Xá»­ lÃ½:**
- DÃ¹ng **Tesseract OCR** / **PaddleOCR** / **EasyOCR**
- Nháº­n diá»‡n tiáº¿ng Anh
- Xá»­ lÃ½ PDF scan vÃ  áº£nh

**CÃ´ng cá»¥:**
- **Tesseract OCR** (tiáº¿ng Anh) - Khuyáº¿n nghá»‹
- **PaddleOCR** (Ä‘a ngÃ´n ngá»¯)
- **EasyOCR** (dá»… sá»­ dá»¥ng)

**Äáº§u vÃ o:**
- PDF scan / Image files

**Äáº§u ra:**
- Raw text (chÆ°a sáº¡ch)

**ğŸ“Œ LÆ°u Ã½:**
OCR cÃ³ thá»ƒ:
- Sai chÃ­nh táº£
- DÃ­nh kÃ½ tá»± rÃ¡c
- Cáº§n lÃ m sáº¡ch á»Ÿ bÆ°á»›c tiáº¿p theo

**Code máº«u:**
```python
import pytesseract
from PIL import Image

def extract_text_from_image(image_path):
    """Extract text from image using Tesseract OCR"""
    image = Image.open(image_path)
    text = pytesseract.image_to_string(image, lang='eng')
    return text
```

---

### ğŸ”¹ BÆ¯á»šC 2 â€“ LÃ m sáº¡ch & chuáº©n hÃ³a vÄƒn báº£n (Text Preprocessing)

**Má»¥c Ä‘Ã­ch:**
- Giáº£m nhiá»…u tá»« OCR
- Chuáº©n bá»‹ cho vector hÃ³a

**CÃ¡c bÆ°á»›c xá»­ lÃ½:**

1. **Lowercase** - Chuyá»ƒn vá» chá»¯ thÆ°á»ng
2. **Remove punctuation** - Loáº¡i bá» dáº¥u cÃ¢u
3. **Remove special characters** - Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
4. **Remove numbers** (náº¿u khÃ´ng cáº§n) - Loáº¡i bá» sá»‘
5. **Remove stopwords** (English) - Loáº¡i bá» tá»« dá»«ng
6. **Lemmatization** (khuyáº¿n nghá»‹) - Chuáº©n hÃ³a tá»« vá» dáº¡ng gá»‘c
7. **Remove short tokens** - Loáº¡i bá» token quÃ¡ ngáº¯n

**CÃ´ng cá»¥:**
- **NLTK** (Natural Language Toolkit)
- **spaCy** (Industrial-strength NLP)

**VÃ­ dá»¥:**
```
Input:  "Machine Learning, is widely used!"
Output: "machine learning widely use"
```

**Äáº§u ra:**
- Clean text

**ğŸ“Œ LÆ°u Ã½:**
- BÆ°á»›c nÃ y áº£nh hÆ°á»Ÿng **Ráº¤T Lá»šN** tá»›i cháº¥t lÆ°á»£ng TF-IDF
- Lemmatization tá»‘t hÆ¡n Stemming (giá»¯ nghÄ©a)

**Code máº«u:**
```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

def preprocess_text(text):
    """Clean and normalize text"""
    # Lowercase
    text = text.lower()
    
    # Remove punctuation and special characters
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    
    # Tokenize
    tokens = text.split()
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]
    
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    
    # Remove short tokens
    tokens = [t for t in tokens if len(t) > 2]
    
    return ' '.join(tokens)
```

---

### ğŸ”¹ BÆ¯á»šC 3 â€“ TrÃ­ch Ä‘áº·c trÆ°ng báº±ng TF-IDF (Æ°u tiÃªn cá»¥m tá»«)

**ğŸ¯ Má»¥c Ä‘Ã­ch:**
- Biá»ƒu diá»…n tÃ i liá»‡u thÃ nh vector sá»‘
- Giá»¯ ngá»¯ cáº£nh báº±ng cá»¥m tá»« (phrase)

**LÃ½ do chá»n TF-IDF:**
- PhÃ¹ há»£p vá»›i clustering
- Dá»… giáº£i thÃ­ch
- Hiá»‡u quáº£ vá»›i táº­p tÃ i liá»‡u khÃ´ng nhÃ£n
- Giá»¯ Ä‘Æ°á»£c ngá»¯ cáº£nh vá»›i n-gram

**ğŸ“ Cáº¥u hÃ¬nh Ä‘Ãºng:**

```python
TfidfVectorizer(
    ngram_range=(1, 2),    # unigram + bigram
    min_df=2,              # loáº¡i cá»¥m quÃ¡ hiáº¿m
    max_df=0.8,            # loáº¡i cá»¥m quÃ¡ phá»• biáº¿n
    stop_words='english',  # loáº¡i stopwords
    norm='l2'              # chuáº©n hÃ³a vector
)
```

**Ã nghÄ©a tham sá»‘:**
- `ngram_range=(1, 2)`: Giá»¯ "machine learning" thay vÃ¬ tÃ¡ch thÃ nh "machine" + "learning"
- `min_df=2`: Loáº¡i cá»¥m xuáº¥t hiá»‡n < 2 láº§n (quÃ¡ hiáº¿m)
- `max_df=0.8`: Loáº¡i cá»¥m xuáº¥t hiá»‡n > 80% documents (quÃ¡ phá»• biáº¿n)
- `norm='l2'`: Chuáº©n hÃ³a vector cho K-means

**CÃ´ng cá»¥:**
- **scikit-learn** (TfidfVectorizer)

**Äáº§u ra:**
- TF-IDF matrix (N_documents Ã— N_features)

**ğŸ“Œ LÆ°u Ã½:**
- N-gram giá»¯ ngá»¯ cáº£nh tá»‘t hÆ¡n unigram
- Chuáº©n hÃ³a L2 quan trá»ng cho K-means

**Code máº«u:**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

def create_tfidf_matrix(documents):
    """Create TF-IDF matrix with n-grams"""
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.8,
        stop_words='english',
        norm='l2'
    )
    
    tfidf_matrix = vectorizer.fit_transform(documents)
    feature_names = vectorizer.get_feature_names_out()
    
    return tfidf_matrix, vectorizer, feature_names
```

---

### ğŸ”¹ BÆ¯á»šC 4 â€“ XÃ¡c Ä‘á»‹nh sá»‘ cá»¥m K báº±ng Elbow

**ğŸ¯ Má»¥c Ä‘Ã­ch:**
- TrÃ¡nh chá»n K cáº£m tÃ­nh
- TÃ¬m sá»‘ cluster há»£p lÃ½

**CÃ¡ch lÃ m:**
1. Cháº¡y K-means vá»›i K = 2 â†’ 10
2. TÃ­nh WCSS (Within-Cluster Sum of Squares) cho má»—i K
3. Váº½ Ä‘á»“ thá»‹ Elbow
4. Chá»n Ä‘iá»ƒm "gÃ£y" (elbow point)

**CÃ´ng cá»¥:**
- **scikit-learn** (KMeans)
- **matplotlib** (váº½ Ä‘á»“ thá»‹)

**Äáº§u ra:**
- GiÃ¡ trá»‹ K tá»‘i Æ°u

**ğŸ“Œ LÆ°u Ã½:**
- Elbow lÃ  heuristic â†’ cÃ³ thá»ƒ káº¿t há»£p domain knowledge
- Äiá»ƒm gÃ£y = nÆ¡i WCSS giáº£m cháº­m láº¡i Ä‘Ã¡ng ká»ƒ

**Code máº«u:**
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

def find_optimal_k_elbow(tfidf_matrix, max_k=10):
    """Find optimal K using Elbow Method"""
    wcss = []
    k_values = range(2, max_k + 1)
    
    for k in k_values:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(tfidf_matrix)
        wcss.append(kmeans.inertia_)
    
    # Plot Elbow curve
    plt.figure(figsize=(10, 6))
    plt.plot(k_values, wcss, 'bo-', linewidth=2, markersize=8)
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
    plt.title('Elbow Method for Optimal K')
    plt.grid(True, alpha=0.3)
    plt.savefig('elbow_curve.png', dpi=150)
    
    # Find elbow point (simplified)
    changes = [wcss[i] - wcss[i+1] for i in range(len(wcss)-1)]
    optimal_k = k_values[changes.index(max(changes)) + 1]
    
    return optimal_k, wcss, k_values
```

---

### ğŸ”¹ BÆ¯á»šC 5 â€“ PhÃ¢n cá»¥m tÃ i liá»‡u báº±ng K-means

**ğŸ¯ Má»¥c Ä‘Ã­ch:**
- Gom tÃ i liá»‡u cÃ³ ná»™i dung tÆ°Æ¡ng tá»±

**CÃ¡ch lÃ m:**
- **Input**: TF-IDF matrix
- **K**: K tá»‘i Æ°u (tá»« Elbow)
- **Khoáº£ng cÃ¡ch**: Cosine (hoáº·c Euclidean sau normalize)

**Thuáº­t toÃ¡n:**
- **K-means Clustering**

**CÃ´ng cá»¥:**
- **scikit-learn** (KMeans)

**Äáº§u ra:**
- Document ID â†’ Cluster ID

**ğŸ“Œ LÆ°u Ã½:**
- Má»—i tÃ i liá»‡u thuá»™c Ä‘Ãºng 1 cluster
- Cosine similarity phÃ¹ há»£p vá»›i text data

**Code máº«u:**
```python
from sklearn.cluster import KMeans

def cluster_documents(tfidf_matrix, n_clusters):
    """Cluster documents using K-means"""
    kmeans = KMeans(
        n_clusters=n_clusters,
        random_state=42,
        n_init=10,
        max_iter=300
    )
    
    cluster_labels = kmeans.fit_predict(tfidf_matrix)
    
    return cluster_labels, kmeans
```

---

### ğŸ”¹ BÆ¯á»šC 6 â€“ TrÃ­ch cá»¥m tá»« Ä‘áº¡i diá»‡n cho tá»«ng cluster

**ğŸ¯ Má»¥c Ä‘Ã­ch:**
- Hiá»ƒu cluster nÃ³i vá» cÃ¡i gÃ¬
- GÃ¡n nhÃ£n / hiá»ƒn thá»‹ cho ngÆ°á»i dÃ¹ng
- Cáº£i thiá»‡n UX

**CÃ¡ch lÃ m (2 lá»±a chá»n):**

#### ğŸ”¹ CÃ¡ch A â€“ TF-IDF top phrase (khuyáº¿n nghá»‹)

1. Láº¥y trung bÃ¬nh TF-IDF cá»§a cÃ¡c document trong cluster
2. Chá»n top bigram/trigram cÃ³ TF-IDF cao nháº¥t

**Æ¯u Ä‘iá»ƒm:**
- Nháº¥t quÃ¡n vá»›i phÆ°Æ¡ng phÃ¡p clustering
- Nhanh vÃ  hiá»‡u quáº£
- Dá»… giáº£i thÃ­ch

#### ğŸ”¹ CÃ¡ch B â€“ RAKE / YAKE

1. Gá»™p text cÃ¡c document trong cluster
2. TrÃ­ch phrase ngá»¯ nghÄ©a báº±ng RAKE hoáº·c YAKE

**Æ¯u Ä‘iá»ƒm:**
- Hiá»ƒu ngá»¯ nghÄ©a tá»‘t hÆ¡n
- TrÃ­ch Ä‘Æ°á»£c cá»¥m tá»« phá»©c táº¡p

**CÃ´ng cá»¥:**
- **scikit-learn** (TF-IDF)
- **RAKE** (Rapid Automatic Keyword Extraction)
- **YAKE** (Yet Another Keyword Extractor)

**Äáº§u ra:**
- Cluster label / Top phrases

**ğŸ“Œ LÆ°u Ã½:**
- BÆ°á»›c nÃ y **KHÃ”NG dÃ¹ng Ä‘á»ƒ clustering**
- Chá»‰ dÃ¹ng Ä‘á»ƒ **giáº£i thÃ­ch** cluster Ä‘Ã£ táº¡o

**Code máº«u:**
```python
def get_top_phrases_per_cluster(tfidf_matrix, cluster_labels, feature_names, n_clusters, top_n=5):
    """Extract top phrases for each cluster"""
    cluster_phrases = {}
    
    for cluster_id in range(n_clusters):
        # Get documents in this cluster
        cluster_docs = tfidf_matrix[cluster_labels == cluster_id]
        
        # Average TF-IDF scores
        avg_tfidf = cluster_docs.mean(axis=0).A1
        
        # Get top phrases
        top_indices = avg_tfidf.argsort()[-top_n:][::-1]
        top_phrases = [feature_names[i] for i in top_indices]
        
        cluster_phrases[cluster_id] = top_phrases
    
    return cluster_phrases
```

---

### ğŸ”¹ BÆ¯á»šC 7 â€“ Embedding ngá»¯ nghÄ©a (cháº¡y SONG SONG)

**ğŸ¯ Má»¥c Ä‘Ã­ch:**
- Hiá»ƒu ngá»¯ nghÄ©a sÃ¢u
- Há»— trá»£ tÃ¬m kiáº¿m vÃ  so sÃ¡nh

**LÃ½ do khÃ´ng thay TF-IDF:**
- **TF-IDF**: Clustering + giáº£i thÃ­ch (dá»±a trÃªn tá»« khÃ³a)
- **Embedding**: Semantic understanding (dá»±a trÃªn ngá»¯ nghÄ©a)
- Hai phÆ°Æ¡ng phÃ¡p **bá»• trá»£** nhau, khÃ´ng thay tháº¿

**Dá»¯ liá»‡u vÃ o:**
- Clean text cá»§a tá»«ng document

**Model:**
- **Sentence-BERT** (all-MiniLM-L6-v2)
- hoáº·c **OpenAI Embedding API** (text-embedding-ada-002)

**Output:**
- Embedding vector 384â€“1536 chiá»u

**CÃ´ng cá»¥:**
- **Sentence-Transformers** (Sentence-BERT)
- **OpenAI API** (Embeddings)

**Äáº§u ra:**
- Embedding vector cho má»—i document

**ğŸ“Œ LÆ°u Ã½:**
- Embedding **KHÃ”NG cháº¡y sau K-means**
- Cháº¡y **SONG SONG** vá»›i TF-IDF pipeline

**Code máº«u:**
```python
from sentence_transformers import SentenceTransformer

def create_embeddings(documents):
    """Create semantic embeddings for documents"""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(documents, show_progress_bar=True)
    return embeddings
```

---

### ğŸ”¹ BÆ¯á»šC 8 â€“ á»¨ng dá»¥ng embedding

#### ğŸ”¹ 8.1 Semantic Search

**CÃ¡ch lÃ m:**
```
User query 
    â†’ Embedding(query) 
    â†’ Cosine similarity vá»›i document embeddings
    â†’ Top documents
```

**Æ¯u Ä‘iá»ƒm:**
- KhÃ´ng cáº§n keyword trÃ¹ng khá»›p 100%
- Hiá»ƒu Ã½ nghÄ©a cÃ¢u há»i

#### ğŸ”¹ 8.2 So sÃ¡nh tÃ i liá»‡u

**á»¨ng dá»¥ng:**
- TÃ¬m tÃ i liá»‡u tÆ°Æ¡ng tá»±
- PhÃ¡t hiá»‡n trÃ¹ng ná»™i dung
- Recommendation system

#### ğŸ”¹ 8.3 (NÃ¢ng cao) RAG

**Retrieval-Augmented Generation:**
- Embedding â†’ Vector DB
- LLM tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u

**Code máº«u:**
```python
from sklearn.metrics.pairwise import cosine_similarity

def semantic_search(query, documents, embeddings, top_k=5):
    """Search documents using semantic similarity"""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Encode query
    query_embedding = model.encode([query])
    
    # Calculate similarity
    similarities = cosine_similarity(query_embedding, embeddings)[0]
    
    # Get top results
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    results = [
        {
            'document': documents[i],
            'similarity': similarities[i],
            'rank': rank + 1
        }
        for rank, i in enumerate(top_indices)
    ]
    
    return results
```

---

## IV. SÆ  Äá»’ Tá»”NG Há»¢P CUá»I CÃ™NG

```
Upload file
    â†“
OCR (BÆ¯á»šC 1)
    â†“
Text preprocessing (BÆ¯á»šC 2)
    â†“
TF-IDF (n-gram) (BÆ¯á»šC 3)
    â†“
Elbow (BÆ¯á»šC 4) â†’ chá»n K
    â†“
K-means (BÆ¯á»šC 5) â†’ Cluster
    â†“
Top phrase (BÆ¯á»šC 6) â†’ Label cluster
(TF-IDF / RAKE)
    â†“
Embedding (BÆ¯á»šC 7) â†’ Search / Similarity / RAG
(cháº¡y song song)
```

---

## V. CÃ”NG Cá»¤ & CÃ”NG NGHá»† Sá»¬ Dá»¤NG

| ThÃ nh pháº§n | CÃ´ng cá»¥ |
|------------|---------|
| **OCR** | Tesseract / PaddleOCR |
| **Preprocessing** | NLTK / spaCy |
| **TF-IDF** | scikit-learn |
| **Clustering** | KMeans (scikit-learn) |
| **Elbow** | matplotlib |
| **Keyword** | TF-IDF / RAKE / YAKE |
| **Embedding** | Sentence-BERT / OpenAI |
| **Similarity** | Cosine similarity |

---

## VI. 3 ÄIá»€U QUAN TRá»ŒNG PHáº¢I NHá»š

### 1. TF-IDF vÃ  Embedding KHÃ”NG thay nhau, mÃ  bá»• trá»£

- **TF-IDF**: PhÃ¢n cá»¥m dá»±a trÃªn tá»« khÃ³a, dá»… giáº£i thÃ­ch
- **Embedding**: Hiá»ƒu ngá»¯ nghÄ©a sÃ¢u, tÃ¬m kiáº¿m thÃ´ng minh

### 2. Clustering â‰  Keyword Extraction

- **Clustering**: Gom tÃ i liá»‡u thÃ nh nhÃ³m
- **Keyword Extraction**: Giáº£i thÃ­ch ná»™i dung nhÃ³m

### 3. Embedding lÃ  Feature Extraction, khÃ´ng pháº£i bÆ°á»›c sau cÃ¹ng

- Embedding cháº¡y **SONG SONG** vá»›i TF-IDF
- KhÃ´ng phá»¥ thuá»™c vÃ o káº¿t quáº£ clustering

---

## VII. VÃ Dá»¤ END-TO-END

### ğŸ”¹ BÆ¯á»šC 0 â€“ Dá»¯ liá»‡u gá»‘c (sau OCR)

**ğŸ“„ Document 1:**
```
Machine learning is widely used in medical image analysis. 
Deep learning models improve diagnosis accuracy.
```

**ğŸ“„ Document 2:**
```
Artificial intelligence and machine learning are applied in healthcare systems. 
Medical data analysis is important.
```

**ğŸ“„ Document 3:**
```
Football players train every day. 
The football team won the championship.
```

### ğŸ”¹ BÆ¯á»šC 1 â€“ Text Preprocessing

**âœ… Clean text:**

**D1:**
```
machine learning widely use medical image analysis deep learning model improve diagnosis accuracy
```

**D2:**
```
artificial intelligence machine learning apply healthcare system medical data analysis important
```

**D3:**
```
football player train every day football team win championship
```

### ğŸ”¹ BÆ¯á»šC 2 â€“ TF-IDF vá»›i n-gram (1,2)

**ğŸ“Š TF-IDF (minh há»a):**

| Phrase | D1 | D2 | D3 |
|--------|----|----|-----|
| machine learning | 0.45 | 0.42 | 0 |
| deep learning | 0.38 | 0 | 0 |
| medical image | 0.33 | 0 | 0 |
| healthcare system | 0 | 0.36 | 0 |
| medical data | 0 | 0.31 | 0 |
| football player | 0 | 0 | 0.41 |
| football team | 0 | 0 | 0.39 |

**ğŸ‘‰ Ngá»¯ cáº£nh Ä‘Æ°á»£c giá»¯ nguyÃªn** (machine learning â‰  machine + learning)

### ğŸ”¹ BÆ¯á»šC 3 â€“ Elbow (chá»n K)

| K | WCSS |
|---|------|
| 1 | 980 |
| 2 | 210 |
| 3 | 190 |
| 4 | 175 |

**ğŸ“‰ Giáº£m máº¡nh nháº¥t tá»« K=1 â†’ K=2, sau Ä‘Ã³ giáº£m ráº¥t Ã­t.**

**ğŸ‘‰ Chá»n K = 2**

### ğŸ”¹ BÆ¯á»šC 4 â€“ K-means Clustering

**ğŸ¯ Káº¿t quáº£ phÃ¢n cá»¥m:**

**Cluster 0 â€“ AI / Healthcare:**
- Document 1
- Document 2

**Cluster 1 â€“ Sports:**
- Document 3

**ğŸ‘‰ Káº¿t quáº£ Ä‘Ãºng trá»±c giÃ¡c con ngÆ°á»i**

### ğŸ”¹ BÆ¯á»šC 5 â€“ Äáº·t tÃªn cluster

**Cluster 0 â€“ Top phrases:**
- machine learning
- medical image analysis
- deep learning
- healthcare system

**ğŸ‘‰ Label: AI in Healthcare**

**Cluster 1 â€“ Top phrases:**
- football player
- football team
- championship

**ğŸ‘‰ Label: Football / Sports**

### ğŸ”¹ BÆ¯á»šC 6 â€“ Embedding (cháº¡y SONG SONG)

**Similarity (cosine):**

| So sÃ¡nh | Similarity |
|---------|------------|
| D1 â†” D2 | 0.89 |
| D1 â†” D3 | 0.12 |
| D2 â†” D3 | 0.10 |

**ğŸ‘‰ Embedding hiá»ƒu:**
- D1 & D2 cÃ¹ng ngá»¯ nghÄ©a
- D3 khÃ¡c hoÃ n toÃ n

### ğŸ”¹ BÆ¯á»šC 7 â€“ Semantic Search

**User query:**
```
AI applications in medical diagnosis
```

**Embedding(query) â†’ so vá»›i document embeddings**

**ğŸ‘‰ Káº¿t quáº£ tráº£ vá»:**
1. Document 1 (similarity: 0.87)
2. Document 2 (similarity: 0.82)

**âŒ KhÃ´ng cáº§n keyword trÃ¹ng khá»›p 100%**

---

## VIII. PROMPT MáºªU (DÃ™NG Vá»šI LLM)

### ğŸ”¹ Prompt 1 â€“ LÃ m sáº¡ch vÄƒn báº£n OCR

```
You are given OCR-extracted English text that may contain noise and formatting errors.

Clean the text by:
- Removing OCR artifacts
- Fixing obvious spacing errors
- Keeping the original meaning

Return only the cleaned text.

Input text:
[OCR_TEXT_HERE]
```

### ğŸ”¹ Prompt 2 â€“ Äáº·t tÃªn cho cluster

```
Given the following key phrases extracted from a document cluster:

- machine learning
- medical image analysis
- deep learning
- healthcare system

Provide a concise and meaningful topic label (max 5 words) for this cluster.
```

### ğŸ”¹ Prompt 3 â€“ TÃ³m táº¯t ná»™i dung cluster

```
You are given a set of documents belonging to the same topic.

Summarize the main theme of these documents in 2â€“3 sentences.
Focus on the core idea.

Documents:
[DOCUMENT_LIST_HERE]
```

### ğŸ”¹ Prompt 4 â€“ Há»— trá»£ semantic search (RAG-ready)

```
Answer the question using only the information provided in the retrieved documents.

If the answer is not present, say "Information not found in the documents".

Question: [USER_QUESTION]

Retrieved documents:
[DOCUMENT_LIST_HERE]
```

---

## IX. TÃ“M Táº®T Äá»€ XUáº¤T (1 Ä‘oáº¡n cho bÃ¡o cÃ¡o)

Há»‡ thá»‘ng Ä‘á» xuáº¥t má»™t **pipeline xá»­ lÃ½ tÃ i liá»‡u tá»± Ä‘á»™ng** bao gá»“m **OCR**, **tiá»n xá»­ lÃ½ vÄƒn báº£n**, **trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng báº±ng TF-IDF vá»›i n-gram** Ä‘á»ƒ phÃ¢n cá»¥m tÃ i liá»‡u thÃ´ng qua **K-means**. Thuáº­t toÃ¡n **Elbow** Ä‘Æ°á»£c sá»­ dá»¥ng nháº±m xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng cá»¥m tá»‘i Æ°u. Äá»“ng thá»i, cÃ¡c **vector embedding ngá»¯ nghÄ©a** Ä‘Æ°á»£c xÃ¢y dá»±ng song song Ä‘á»ƒ há»— trá»£ cÃ¡c tÃ¡c vá»¥ **tÃ¬m kiáº¿m vÃ  so sÃ¡nh ná»™i dung**. Viá»‡c káº¿t há»£p TF-IDF vÃ  embedding giÃºp há»‡ thá»‘ng vá»«a Ä‘áº£m báº£o kháº£ nÄƒng diá»…n giáº£i, vá»«a náº¯m báº¯t Ä‘Æ°á»£c ngá»¯ nghÄ©a sÃ¢u cá»§a tÃ i liá»‡u.

---

## X. CÃ‚U CHá»T (Ráº¤T HAY DÃ™NG KHI Báº¢O Vá»†)

> **TF-IDF vá»›i n-gram** Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ biá»ƒu diá»…n tÃ i liá»‡u á»Ÿ má»©c tá»« vÃ  cá»¥m tá»« nháº±m phá»¥c vá»¥ **phÃ¢n cá»¥m vÃ  diá»…n giáº£i ná»™i dung**, trong khi **embedding** Ä‘Æ°á»£c xÃ¢y dá»±ng song song Ä‘á»ƒ há»— trá»£ cÃ¡c tÃ¡c vá»¥ **ngá»¯ nghÄ©a** nhÆ° tÃ¬m kiáº¿m vÃ  so sÃ¡nh tÃ i liá»‡u.

---

## XI. CHECKLIST TRIá»‚N KHAI

### Giai Ä‘oáº¡n 1: Chuáº©n bá»‹ dá»¯ liá»‡u
- [ ] CÃ i Ä‘áº·t Tesseract OCR
- [ ] CÃ i Ä‘áº·t NLTK vÃ  download stopwords
- [ ] Chuáº©n bá»‹ táº­p tÃ i liá»‡u test

### Giai Ä‘oáº¡n 2: TF-IDF & Clustering
- [ ] Implement text preprocessing
- [ ] Táº¡o TF-IDF matrix vá»›i n-gram
- [ ] Implement Elbow Method
- [ ] Cháº¡y K-means clustering
- [ ] TrÃ­ch xuáº¥t top phrases

### Giai Ä‘oáº¡n 3: Embedding & Search
- [ ] CÃ i Ä‘áº·t Sentence-Transformers
- [ ] Táº¡o embeddings cho documents
- [ ] Implement semantic search
- [ ] Test similarity calculation

### Giai Ä‘oáº¡n 4: ÄÃ¡nh giÃ¡ & Tá»‘i Æ°u
- [ ] ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng clustering
- [ ] Kiá»ƒm tra top phrases cÃ³ Ã½ nghÄ©a
- [ ] Test semantic search accuracy
- [ ] Tá»‘i Æ°u tham sá»‘

---

**TÃ¡c giáº£**: Kiro AI Assistant  
**NgÃ y**: 2026-02-03  
**Version**: 1.0  
**Status**: âœ… HOÃ€N CHá»ˆNH
