# ğŸ“‹ BÃO CÃO AUDIT PIPELINE Xá»¬ LÃ TÃ€I LIá»†U

**NgÃ y audit**: 2026-02-03  
**Auditor**: Kiro AI Assistant  
**Há»‡ thá»‘ng**: Visual Language Tutor - Python API

---

## ğŸ¯ Tá»”NG QUAN

Audit há»‡ thá»‘ng theo checklist pipeline chuáº©n:
- OCR â†’ Text Preprocessing â†’ TF-IDF â†’ Elbow â†’ K-means â†’ Keyword Extraction â†’ Embedding

---

## âœ… Káº¾T QUáº¢ Tá»”NG THá»‚

| TiÃªu chÃ­ | Tráº¡ng thÃ¡i | Ghi chÃº |
|----------|------------|---------|
| **OCR trÆ°á»›c NLP** | âœ… ÄÃšNG | OCR trong upload endpoint |
| **TF-IDF cÃ³ n-gram** | âœ… ÄÃšNG | ngram_range=(1,3) |
| **Elbow chá»n K** | âœ… ÄÃšNG | CÃ³ implement Ä‘áº§y Ä‘á»§ |
| **K-means trÃªn TF-IDF** | âœ… ÄÃšNG | Sá»­ dá»¥ng TF-IDF matrix |
| **Giáº£i thÃ­ch cluster** | âš ï¸ THIáº¾U | ChÆ°a cÃ³ bÆ°á»›c nÃ y |
| **Embedding cho search** | âš ï¸ THIáº¾U | ChÆ°a implement |
| **KhÃ´ng trá»™n vai trÃ²** | âœ… ÄÃšNG | Logic rÃµ rÃ ng |

**Äiá»ƒm tá»•ng**: 5/7 âœ… | 2/7 âš ï¸

---

## ğŸ“Š CHI TIáº¾T Tá»ªNG BÆ¯á»šC

### ğŸ”· 1. Giai Ä‘oáº¡n INPUT & OCR

#### âœ”ï¸ Dá»¯ liá»‡u Ä‘áº§u vÃ o

**File**: `python-api/main.py` (line 550+)

```python
@app.post("/api/upload-document")
async def upload_document(
    file: UploadFile = File(...),
    max_words: int = Form(20),
    language: str = Form("en")
):
```

**âœ… ÄÃšNG:**
- Upload PDF scan / image / text
- LÆ°u metadata (filename, timestamp)
- Táº¡o document_id unique

#### âœ”ï¸ OCR

**File**: `python-api/main.py` (line 520+)

```python
def extract_text_from_file(file_path: str) -> str:
    """Extract text from uploaded file"""
    file_ext = Path(file_path).suffix.lower()
    
    if file_ext == '.txt':
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    elif file_ext == '.pdf' and PDF_SUPPORT:
        text = ""
        with open(file_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text
    
    elif file_ext in ['.docx', '.doc'] and PDF_SUPPORT:
        doc = docx.Document(file_path)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        return text
```

**âœ… ÄÃšNG:**
- DÃ¹ng PyPDF2 cho PDF
- DÃ¹ng python-docx cho DOCX
- OCR cháº¡y TRÆ¯á»šC táº¥t cáº£ NLP
- Output lÃ  raw text

**ğŸ“Œ LÆ°u Ã½:**
- PyPDF2 khÃ´ng pháº£i OCR thá»±c sá»± (chá»‰ extract text layer)
- Náº¿u cáº§n OCR cho áº£nh scan â†’ cáº§n thÃªm Tesseract/PaddleOCR

---

### ğŸ”· 2. Text Preprocessing

**File**: `python-api/ensemble_extractor.py` (line 60+)

```python
def tokenize_text(text: str, remove_stopwords: bool = True) -> List[str]:
    """Tokenize vÃ  lÃ m sáº¡ch text"""
    # Lowercase vÃ  tokenize
    tokens = word_tokenize(text.lower())
    
    # Remove punctuation vÃ  short words
    tokens = [t for t in tokens if t.isalnum() and len(t) > 2]
    
    # Remove stopwords
    if remove_stopwords:
        tokens = [t for t in tokens if t not in ENGLISH_STOPWORDS]
    
    return tokens
```

**âœ… ÄÃšNG:**
- âœ… Lowercase
- âœ… Remove punctuation
- âœ… Remove stopwords (English)
- âœ… Remove short tokens (< 3 chars)

**âš ï¸ THIáº¾U:**
- âŒ Lemmatization (chÆ°a tháº¥y sá»­ dá»¥ng)
- âŒ Remove numbers (cÃ³ thá»ƒ cáº§n)

**ğŸ“Œ ÄÃ¡nh giÃ¡:**
- Preprocessing cÆ¡ báº£n Ä‘Ã£ cÃ³
- NÃªn thÃªm lemmatization Ä‘á»ƒ chuáº©n hÃ³a tá»‘t hÆ¡n

---

### ğŸ”· 3. TF-IDF Feature Extraction

**File**: `python-api/ensemble_extractor.py` (line 130+)

```python
def calculate_tfidf(documents: List[str]) -> Dict[str, float]:
    """TÃ­nh TF-IDF scores"""
    vectorizer = TfidfVectorizer(
        max_features=1000,
        ngram_range=(1, 3),      # âœ… ÄÃšNG: unigram + bigram + trigram
        stop_words='english'     # âœ… ÄÃšNG: loáº¡i stopwords
    )
    
    tfidf_matrix = vectorizer.fit_transform(documents)
    feature_names = vectorizer.get_feature_names_out()
```

**âœ… ÄÃšNG:**
- âœ… DÃ¹ng TF-IDF (khÃ´ng pháº£i CountVectorizer)
- âœ… CÃ³ n-gram: (1, 3) - unigram, bigram, trigram
- âœ… stop_words='english'
- âœ… max_features=1000 (giá»›i háº¡n features)

**âš ï¸ Cáº¦N Cáº¢I THIá»†N:**
- âŒ Thiáº¿u `min_df` (nÃªn cÃ³ min_df=2)
- âŒ Thiáº¿u `max_df` (nÃªn cÃ³ max_df=0.8)
- âŒ Thiáº¿u `norm='l2'` (quan trá»ng cho K-means)

**ğŸ“Œ Äá» xuáº¥t:**
```python
vectorizer = TfidfVectorizer(
    ngram_range=(1, 2),      # Giáº£m xuá»‘ng (1,2) cho hiá»‡u quáº£
    min_df=2,                # âœ… THÃŠM: loáº¡i cá»¥m quÃ¡ hiáº¿m
    max_df=0.8,              # âœ… THÃŠM: loáº¡i cá»¥m quÃ¡ phá»• biáº¿n
    stop_words='english',
    norm='l2',               # âœ… THÃŠM: chuáº©n hÃ³a cho K-means
    max_features=1000
)
```

---

### ğŸ”· 4. Elbow Method

**File**: `python-api/kmeans_clustering.py` (line 15+)

```python
def calculate_elbow(tfidf_matrix: np.ndarray, max_k: int = 10) -> Tuple[int, List[float], List[int]]:
    """
    Elbow Method Ä‘á»ƒ tÃ¬m sá»‘ cluster tá»‘i Æ°u
    """
    print(f"[Elbow] Testing K from 2 to {max_k}...")
    
    inertias = []
    k_values = list(range(2, min(max_k + 1, tfidf_matrix.shape[0])))
    
    for k in k_values:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(tfidf_matrix)
        inertias.append(kmeans.inertia_)
        print(f"[Elbow] K={k}, Inertia={kmeans.inertia_:.2f}")
    
    # TÃ¬m elbow point
    if len(inertias) >= 3:
        changes = []
        for i in range(1, len(inertias)):
            change = abs(inertias[i] - inertias[i-1])
            changes.append(change)
        
        max_change_idx = changes.index(max(changes))
        optimal_k = k_values[max_change_idx + 1]
```

**âœ… ÄÃšNG:**
- âœ… Cháº¡y K-means vá»›i nhiá»u K (2-10)
- âœ… TÃ­nh WCSS/inertia cho má»—i K
- âœ… Váº½ Ä‘á»“ thá»‹ Elbow (cÃ³ function `plot_elbow_curve`)
- âœ… Chá»n K táº¡i Ä‘iá»ƒm giáº£m máº¡nh nháº¥t
- âœ… CÃ³ ghi chÃº Elbow lÃ  heuristic

**ğŸ“Œ ÄÃ¡nh giÃ¡:**
- Implementation hoÃ n háº£o âœ…
- CÃ³ visualization
- Logic chá»n K há»£p lÃ½

---

### ğŸ”· 5. K-means Clustering

**File**: `python-api/kmeans_clustering.py` (line 90+)

```python
def cluster_vocabulary_kmeans(
    vocabulary_list: List[Dict],
    text: str,
    n_clusters: int = None,
    use_elbow: bool = True,
    max_k: int = 10,
    document_id: str = None
) -> Dict:
    """Cluster tá»« vá»±ng sá»­ dá»¥ng K-Means"""
    
    # Táº¡o TF-IDF vectors cho tá»« vá»±ng
    words = [v['word'] for v in vocabulary_list]
    
    # Táº¡o documents cho má»—i tá»«
    from nltk.tokenize import sent_tokenize
    sentences = sent_tokenize(text)
    
    word_documents = []
    for word in words:
        word_sentences = [s for s in sentences if word.lower() in s.lower()]
        if word_sentences:
            word_documents.append(' '.join(word_sentences[:3]))
        else:
            word_documents.append(word)
    
    # Táº¡o TF-IDF matrix
    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(word_documents)
    
    # XÃ¡c Ä‘á»‹nh sá»‘ cluster
    if n_clusters is None and use_elbow:
        optimal_k, inertias, k_values = calculate_elbow(tfidf_matrix.toarray(), max_k)
        n_clusters = optimal_k
    
    # Cháº¡y K-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(tfidf_matrix)
```

**âœ… ÄÃšNG:**
- âœ… DÃ¹ng K-means (scikit-learn)
- âœ… Input lÃ  TF-IDF matrix
- âœ… K tá»« Elbow Method
- âœ… Má»—i document thuá»™c 1 cluster

**âš ï¸ Váº¤N Äá»€:**
- âš ï¸ Táº¡o TF-IDF matrix Má»šI trong clustering (khÃ´ng dÃ¹ng TF-IDF tá»« ensemble)
- âš ï¸ CÃ³ thá»ƒ gÃ¢y inconsistency

**ğŸ“Œ LÆ°u Ã½:**
- Logic Ä‘Ãºng nhÆ°ng cÃ³ 2 TF-IDF riÃªng biá»‡t:
  1. TF-IDF trong `ensemble_extractor` (cho scoring)
  2. TF-IDF trong `kmeans_clustering` (cho clustering)
- NÃªn thá»‘ng nháº¥t hoáº·c giáº£i thÃ­ch rÃµ

---

### ğŸ”· 6. Giáº£i thÃ­ch & Äáº·t tÃªn cluster

**File**: `python-api/kmeans_clustering.py` (line 150+)

```python
# BÆ°á»›c 4: Tá»• chá»©c káº¿t quáº£ theo cluster
clusters = {}
for idx, label in enumerate(cluster_labels):
    if label not in clusters:
        clusters[label] = []
    
    clusters[label].append({
        'word': vocabulary_list[idx]['word'],
        'score': vocabulary_list[idx]['score'],
        'cluster_id': int(label)
    })

# Sáº¯p xáº¿p tá»« trong má»—i cluster theo score
for label in clusters:
    clusters[label] = sorted(clusters[label], key=lambda x: x['score'], reverse=True)

# Chá»n Ä‘áº¡i diá»‡n cho má»—i cluster
cluster_representatives = []
for label in sorted(clusters.keys()):
    representative = clusters[label][0]
    cluster_representatives.append({
        'cluster_id': int(label),
        'representative_word': representative['word'],
        'representative_score': representative['score'],
        'cluster_size': len(clusters[label]),
        'words': [w['word'] for w in clusters[label][:5]]  # Top 5 words
    })
```

**âš ï¸ THIáº¾U:**
- âŒ KhÃ´ng cÃ³ bÆ°á»›c trÃ­ch keyword/phrase cho cluster
- âŒ KhÃ´ng cÃ³ label/mÃ´ táº£ cluster
- âŒ Chá»‰ chá»n representative word (tá»« cÃ³ score cao nháº¥t)

**âœ… CÃ“:**
- âœ… Tá»• chá»©c theo cluster
- âœ… Chá»n top words trong cluster

**ğŸ“Œ Äá» xuáº¥t:**
Cáº§n thÃªm function:
```python
def extract_cluster_keywords(cluster_documents, top_n=5):
    """TrÃ­ch top keywords cho cluster báº±ng TF-IDF"""
    vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=50)
    tfidf = vectorizer.fit_transform(cluster_documents)
    
    # Get top features
    avg_tfidf = tfidf.mean(axis=0).A1
    top_indices = avg_tfidf.argsort()[-top_n:][::-1]
    feature_names = vectorizer.get_feature_names_out()
    
    return [feature_names[i] for i in top_indices]
```

---

### ğŸ”· 7. Embedding (CHáº Y SONG SONG)

**TÃ¬m kiáº¿m trong code...**

**âŒ KHÃ”NG TÃŒM THáº¤Y:**
- KhÃ´ng cÃ³ implementation Sentence-BERT
- KhÃ´ng cÃ³ OpenAI Embedding
- KhÃ´ng cÃ³ semantic search

**ğŸ“Œ ÄÃ¡nh giÃ¡:**
- Há»‡ thá»‘ng CHÆ¯A cÃ³ embedding
- ÄÃ¢y lÃ  gap lá»›n trong pipeline

**ğŸ“Œ Äá» xuáº¥t:**
Cáº§n thÃªm:
```python
from sentence_transformers import SentenceTransformer

def create_embeddings(documents):
    """Create semantic embeddings"""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(documents)
    return embeddings

def semantic_search(query, documents, embeddings, top_k=5):
    """Search using embeddings"""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    query_embedding = model.encode([query])
    
    from sklearn.metrics.pairwise import cosine_similarity
    similarities = cosine_similarity(query_embedding, embeddings)[0]
    
    top_indices = similarities.argsort()[-top_k:][::-1]
    return [(documents[i], similarities[i]) for i in top_indices]
```

---

### ğŸ”· 8. á»¨ng dá»¥ng cuá»‘i

**File**: `python-api/main.py`

**âœ… CÃ“:**
- âœ… Upload document endpoint
- âœ… Vocabulary extraction
- âœ… K-means clustering
- âœ… Response JSON vá»›i clusters

**âŒ THIáº¾U:**
- âŒ Semantic search endpoint
- âŒ Document similarity
- âŒ Recommendation

---

## ğŸ” KIá»‚M TRA LOGIC Tá»”NG THá»‚

### âœ… Clustering â‰  Keyword Extraction

**ÄÃšNG** âœ…

- Clustering: `kmeans_clustering.py` - gom documents
- Keyword: `ensemble_extractor.py` - trÃ­ch tá»« vá»±ng
- Vai trÃ² rÃµ rÃ ng, khÃ´ng trá»™n láº«n

### âœ… TF-IDF â‰  Embedding

**ÄÃšNG** âœ… (nhÆ°ng thiáº¿u embedding)

- TF-IDF: CÃ³ implement Ä‘áº§y Ä‘á»§
- Embedding: ChÆ°a cÃ³
- KhÃ´ng trá»™n láº«n (vÃ¬ chÆ°a cÃ³ embedding)

### âœ… Elbow â‰  Thuáº­t toÃ¡n tá»‘i Æ°u tuyá»‡t Ä‘á»‘i

**ÄÃšNG** âœ…

- CÃ³ comment: "Elbow lÃ  heuristic"
- CÃ³ thá»ƒ káº¿t há»£p domain knowledge

### âœ… Pipeline cÃ³ thá»ƒ giáº£i thÃ­ch

**ÄÃšNG** âœ…

- Code cÃ³ comment rÃµ rÃ ng
- CÃ³ documentation Ä‘áº§y Ä‘á»§
- Logic dá»… hiá»ƒu

---

## ğŸŸ¢ CHECKLIST Tá»”NG Káº¾T NHANH

| CÃ¢u há»i | Tráº£ lá»i | Ghi chÃº |
|---------|---------|---------|
| **OCR trÆ°á»›c NLP?** | âœ… CÃ“ | Extract text trÆ°á»›c khi NLP |
| **TF-IDF cÃ³ n-gram?** | âœ… CÃ“ | ngram_range=(1,3) |
| **Elbow chá»n K?** | âœ… CÃ“ | CÃ³ Ä‘áº§y Ä‘á»§ |
| **K-means cháº¡y trÃªn TF-IDF?** | âœ… CÃ“ | ÄÃºng input |
| **CÃ³ giáº£i thÃ­ch cluster?** | âš ï¸ THIáº¾U | Chá»‰ cÃ³ representative |
| **Embedding cho search?** | âŒ KHÃ”NG | ChÆ°a implement |
| **KhÃ´ng trá»™n vai trÃ²?** | âœ… CÃ“ | Logic rÃµ rÃ ng |

**Káº¿t quáº£**: 5/7 âœ…

---

## ğŸ“ ÄÃNH GIÃ Tá»”NG THá»‚

### ğŸŸ¢ Äiá»ƒm máº¡nh

1. **Pipeline cÆ¡ báº£n Ä‘Ãºng** âœ…
   - OCR â†’ Preprocessing â†’ TF-IDF â†’ Elbow â†’ K-means
   - Thá»© tá»± logic Ä‘Ãºng

2. **TF-IDF implementation tá»‘t** âœ…
   - CÃ³ n-gram (1,3)
   - CÃ³ stopwords filtering
   - Code rÃµ rÃ ng

3. **Elbow Method hoÃ n háº£o** âœ…
   - Logic Ä‘Ãºng
   - CÃ³ visualization
   - CÃ³ documentation

4. **K-means Ä‘Ãºng** âœ…
   - Sá»­ dá»¥ng TF-IDF matrix
   - CÃ³ Silhouette Score
   - CÃ³ cluster organization

5. **Documentation xuáº¥t sáº¯c** âœ…
   - Nhiá»u file MD chi tiáº¿t
   - Code cÃ³ comment
   - Dá»… hiá»ƒu

### ğŸŸ¡ Äiá»ƒm cáº§n cáº£i thiá»‡n

1. **TF-IDF configuration** âš ï¸
   - Thiáº¿u `min_df`, `max_df`
   - Thiáº¿u `norm='l2'`
   - NÃªn cáº£i thiá»‡n

2. **Preprocessing** âš ï¸
   - Thiáº¿u lemmatization
   - CÃ³ thá»ƒ tá»‘t hÆ¡n

3. **Cluster explanation** âš ï¸
   - Chá»‰ cÃ³ representative word
   - ChÆ°a cÃ³ top keywords/phrases
   - ChÆ°a cÃ³ label

### ğŸ”´ Gap lá»›n

1. **Embedding HOÃ€N TOÃ€N THIáº¾U** âŒ
   - KhÃ´ng cÃ³ Sentence-BERT
   - KhÃ´ng cÃ³ semantic search
   - KhÃ´ng cÃ³ similarity
   - ÄÃ¢y lÃ  gap nghiÃªm trá»ng

2. **OCR thá»±c sá»±** âš ï¸
   - PyPDF2 khÃ´ng pháº£i OCR
   - Cáº§n Tesseract cho áº£nh scan

---

## ğŸ¯ Äá»€ XUáº¤T HÃ€NH Äá»˜NG

### Æ¯u tiÃªn CAO (Báº¯t buá»™c)

1. **ThÃªm Embedding** ğŸ”´
   ```python
   # CÃ i Ä‘áº·t
   pip install sentence-transformers
   
   # Implement
   - create_embeddings()
   - semantic_search()
   - document_similarity()
   ```

2. **Cáº£i thiá»‡n TF-IDF config** ğŸŸ¡
   ```python
   TfidfVectorizer(
       ngram_range=(1, 2),
       min_df=2,           # THÃŠM
       max_df=0.8,         # THÃŠM
       norm='l2',          # THÃŠM
       stop_words='english'
   )
   ```

3. **ThÃªm Cluster Explanation** ğŸŸ¡
   ```python
   def get_cluster_keywords(cluster_docs, top_n=5):
       # Extract top TF-IDF phrases
       pass
   ```

### Æ¯u tiÃªn TRUNG (NÃªn cÃ³)

4. **ThÃªm Lemmatization** ğŸŸ¡
   ```python
   from nltk.stem import WordNetLemmatizer
   lemmatizer = WordNetLemmatizer()
   tokens = [lemmatizer.lemmatize(t) for t in tokens]
   ```

5. **ThÃªm OCR thá»±c sá»±** ğŸŸ¡
   ```python
   import pytesseract
   # For image files
   text = pytesseract.image_to_string(image)
   ```

### Æ¯u tiÃªn THáº¤P (Nice to have)

6. **Thá»‘ng nháº¥t TF-IDF** ğŸŸ¢
   - DÃ¹ng chung 1 TF-IDF matrix
   - TrÃ¡nh táº¡o 2 láº§n

7. **ThÃªm tests** ğŸŸ¢
   - Unit tests cho tá»«ng function
   - Integration tests cho pipeline

---

## ğŸ“Š ÄIá»‚M Sá» CUá»I CÃ™NG

| TiÃªu chÃ­ | Äiá»ƒm | Tá»‘i Ä‘a |
|----------|------|--------|
| OCR & Input | 8 | 10 |
| Preprocessing | 7 | 10 |
| TF-IDF | 8 | 10 |
| Elbow Method | 10 | 10 |
| K-means | 9 | 10 |
| Cluster Explanation | 4 | 10 |
| Embedding | 0 | 10 |
| Documentation | 10 | 10 |

**Tá»”NG ÄIá»‚M**: **56/80** (70%)

**ÄÃNH GIÃ**: ğŸŸ¡ **KHÃ** - Cáº§n cáº£i thiá»‡n

---

## âœ… Káº¾T LUáº¬N

### Há»‡ thá»‘ng hiá»‡n táº¡i:

âœ… **Pipeline cÆ¡ báº£n ÄÃšNG**  
âœ… **TF-IDF + K-means hoáº¡t Ä‘á»™ng tá»‘t**  
âœ… **Elbow Method hoÃ n háº£o**  
âœ… **Documentation xuáº¥t sáº¯c**  

âš ï¸ **Thiáº¿u Embedding (gap lá»›n)**  
âš ï¸ **Thiáº¿u Cluster Explanation**  
âš ï¸ **TF-IDF config cáº§n cáº£i thiá»‡n**  

### Khuyáº¿n nghá»‹:

1. **THÃŠM EMBEDDING NGAY** - ÄÃ¢y lÃ  gap nghiÃªm trá»ng
2. Cáº£i thiá»‡n TF-IDF configuration
3. ThÃªm cluster explanation
4. ThÃªm lemmatization

### CÃ³ thá»ƒ báº£o vá»‡ Ä‘Æ°á»£c khÃ´ng?

**CÃ“** âœ… - NhÆ°ng cáº§n giáº£i thÃ­ch:
- Táº¡i sao chÆ°a cÃ³ embedding (future work)
- Táº­p trung vÃ o TF-IDF + K-means
- CÃ³ roadmap Ä‘á»ƒ thÃªm embedding

---

**NgÆ°á»i audit**: Kiro AI Assistant  
**NgÃ y**: 2026-02-03  
**Version**: 1.0  
**Status**: âœ… HOÃ€N THÃ€NH
