"""
STAGE 2 ‚Äì Context Intelligence Engine
Ch·ªçn c√¢u ng·ªØ c·∫£nh t·ªët nh·∫•t cho m·ªói t·ª´ v·ª±ng

Pipeline:
1. Build Sentence objects v·ªõi metadata
2. Map t·ª´ v·ª±ng ‚Üí danh s√°ch c√¢u ch·ª©a t·ª´
3. L·ªçc c√¢u kh√¥ng h·ª£p l·ªá (qu√° ng·∫Øn/d√†i, kh√¥ng c√≥ ƒë·ªông t·ª´)
4. Ch·∫•m ƒëi·ªÉm c√¢u theo c√¥ng th·ª©c weighted
5. Ch·ªçn c√¢u t·ªët nh·∫•t cho m·ªói t·ª´
6. Highlight t·ª´ v·ª±ng trong c√¢u

Th∆∞ vi·ªán s·ª≠ d·ª•ng:
- spaCy: Ph√¢n t√≠ch c√∫ ph√°p, POS tagging, dependency parsing
- NLTK: Sentence tokenization, stopwords
- scikit-learn: TF-IDF vectorization
- underthesea: Vietnamese NLP (n·∫øu c·∫ßn)
"""

import re
import math
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
import spacy
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import nltk

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Load spaCy models
try:
    nlp_en = spacy.load("en_core_web_sm")
except OSError:
    print("‚ö†Ô∏è  English model not found. Run: python -m spacy download en_core_web_sm")
    nlp_en = None

# Vietnamese support (optional)
try:
    from underthesea import word_tokenize as vi_tokenize
    HAS_VIETNAMESE = True
except ImportError:
    print("‚ö†Ô∏è  Vietnamese support not available. Install: pip install underthesea")
    HAS_VIETNAMESE = False


# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class Sentence:
    """C·∫•u tr√∫c c√¢u v·ªõi metadata"""
    sentence_id: str
    text: str
    position: int
    word_count: int
    has_verb: bool
    paragraph_id: str = "Unknown"
    section_title: str = "Unknown"
    tokens: List[str] = field(default_factory=list)
    pos_tags: List[str] = field(default_factory=list)


@dataclass
class SentenceScore:
    """ƒêi·ªÉm s·ªë c·ªßa c√¢u"""
    sentence_id: str
    score: float
    breakdown: Dict[str, float]


@dataclass
class VocabularyContext:
    """Ng·ªØ c·∫£nh cho t·ª´ v·ª±ng"""
    word: str
    final_score: float
    context_sentence: str  # C√¢u v·ªõi t·ª´ ƒë∆∞·ª£c highlight
    sentence_id: str
    sentence_score: float
    explanation: str = ""


# ============================================================================
# B∆Ø·ªöC 2.1 ‚Äì X√ÇY D·ª∞NG SENTENCE OBJECTS
# ============================================================================

def build_sentences(text: str, language: str = "en") -> List[Sentence]:
    """
    T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u v√† t·∫°o Sentence objects
    
    Args:
        text: VƒÉn b·∫£n ƒë·∫ßu v√†o
        language: Ng√¥n ng·ªØ ('en' ho·∫∑c 'vi')
    
    Returns:
        Danh s√°ch Sentence objects
    """
    # Tokenize sentences
    if language == "vi" and HAS_VIETNAMESE:
        # Vietnamese sentence tokenization
        sentences_text = re.split(r'[.!?]+', text)
        sentences_text = [s.strip() for s in sentences_text if s.strip()]
    else:
        # English sentence tokenization
        sentences_text = sent_tokenize(text)
    
    sentences = []
    
    for idx, sent_text in enumerate(sentences_text):
        # Tokenize words
        if language == "vi" and HAS_VIETNAMESE:
            tokens = vi_tokenize(sent_text)
        else:
            tokens = sent_text.split()
        
        word_count = len(tokens)
        
        # Detect verb using spaCy
        has_verb = detect_verb_spacy(sent_text, language)
        
        # POS tagging
        pos_tags = []
        if nlp_en and language == "en":
            doc = nlp_en(sent_text)
            pos_tags = [token.pos_ for token in doc]
        
        sentence = Sentence(
            sentence_id=f"s{idx + 1}",
            text=sent_text,
            position=idx,
            word_count=word_count,
            has_verb=has_verb,
            paragraph_id=f"p{idx // 5 + 1}",  # Gi·∫£ ƒë·ªãnh m·ªói ƒëo·∫°n ~5 c√¢u
            section_title="Unknown",
            tokens=tokens,
            pos_tags=pos_tags
        )
        
        sentences.append(sentence)
    
    return sentences


def detect_verb_spacy(text: str, language: str = "en") -> bool:
    """
    Ph√°t hi·ªán ƒë·ªông t·ª´ trong c√¢u s·ª≠ d·ª•ng spaCy
    
    Args:
        text: C√¢u c·∫ßn ki·ªÉm tra
        language: Ng√¥n ng·ªØ
    
    Returns:
        True n·∫øu c√≥ ƒë·ªông t·ª´
    """
    if language == "en" and nlp_en:
        doc = nlp_en(text)
        for token in doc:
            if token.pos_ == "VERB":
                return True
    
    # Fallback: heuristic cho ti·∫øng Vi·ªát
    if language == "vi":
        vietnamese_verbs = [
            'l√†', 'c√≥', 'ƒë∆∞·ª£c', 'l√†m', 'ƒëi', 'ƒë·∫øn', 'v·ªÅ', 'ra', 'v√†o', 'l√™n', 'xu·ªëng',
            'cho', 'l·∫•y', 'ƒë∆∞a', 'mang', 'c·∫ßm', 'n√≥i', 'k·ªÉ', 'h·ªèi', 'tr·∫£ l·ªùi',
            'nghƒ©', 'bi·∫øt', 'hi·ªÉu', 'h·ªçc', 'd·∫°y', 'vi·∫øt', 'ƒë·ªçc', 'nghe', 'nh√¨n', 'th·∫•y',
            'ƒÉn', 'u·ªëng', 'ng·ªß', 'th·ª©c', 'ch∆°i', 'l√†m vi·ªác', 'ngh·ªâ', 'ch·∫°y', 'ƒë·ª©ng', 'ng·ªìi',
            'ph√°t tri·ªÉn', 't·∫°o', 'x√¢y d·ª±ng', 'thi·∫øt k·∫ø', 'th·ª±c hi·ªán', '√°p d·ª•ng', 's·ª≠ d·ª•ng',
            'nghi√™n c·ª©u', 'ph√¢n t√≠ch', 'ƒë√°nh gi√°', 'ki·ªÉm tra', 'th·ª≠ nghi·ªám', 'c·∫£i thi·ªán'
        ]
        text_lower = text.lower()
        return any(verb in text_lower for verb in vietnamese_verbs)
    
    return False


# ============================================================================
# B∆Ø·ªöC 2.2 ‚Äì G√ÅN T·ª™ ‚Üí DANH S√ÅCH C√ÇU CH·ª®A T·ª™
# ============================================================================

def map_words_to_sentences(
    vocabulary_words: List[str],
    sentences: List[Sentence]
) -> Dict[str, List[str]]:
    """
    T·∫°o map t·ª´ v·ª±ng ‚Üí danh s√°ch c√¢u ch·ª©a t·ª´ ƒë√≥
    Case-insensitive, whole-word matching
    
    Args:
        vocabulary_words: Danh s√°ch t·ª´ v·ª±ng
        sentences: Danh s√°ch c√¢u
    
    Returns:
        Dictionary {word: [sentence_ids]}
    """
    word_map = {}
    
    for word in vocabulary_words:
        sentence_ids = []
        word_lower = word.lower()
        
        # T·∫°o regex cho whole-word matching
        word_pattern = re.compile(r'\b' + re.escape(word_lower) + r'\b', re.IGNORECASE)
        
        for sentence in sentences:
            if word_pattern.search(sentence.text):
                sentence_ids.append(sentence.sentence_id)
        
        word_map[word] = sentence_ids
    
    return word_map


# ============================================================================
# B∆Ø·ªöC 2.3 ‚Äì L·ªåC C√ÇU R√ÅC
# ============================================================================

def filter_invalid_sentences(
    sentences: List[Sentence],
    min_words: int = 5,
    max_words: int = 35,
    require_verb: bool = True
) -> List[Sentence]:
    """
    L·ªçc c√°c c√¢u kh√¥ng h·ª£p l·ªá
    
    Args:
        sentences: Danh s√°ch c√¢u
        min_words: S·ªë t·ª´ t·ªëi thi·ªÉu
        max_words: S·ªë t·ª´ t·ªëi ƒëa
        require_verb: Y√™u c·∫ßu c√≥ ƒë·ªông t·ª´
    
    Returns:
        Danh s√°ch c√¢u h·ª£p l·ªá
    """
    valid_sentences = []
    
    for sentence in sentences:
        # Lo·∫°i b·ªè c√¢u qu√° ng·∫Øn
        if sentence.word_count < min_words:
            continue
        
        # Lo·∫°i b·ªè c√¢u qu√° d√†i
        if sentence.word_count > max_words:
            continue
        
        # Lo·∫°i b·ªè c√¢u kh√¥ng c√≥ ƒë·ªông t·ª´
        if require_verb and not sentence.has_verb:
            continue
        
        # Lo·∫°i b·ªè c√¢u ch·ªâ l√† ti√™u ƒë·ªÅ (to√†n ch·ªØ hoa)
        upper_count = sum(1 for c in sentence.text if c.isupper())
        if upper_count / len(sentence.text) > 0.7:
            continue
        
        # Lo·∫°i b·ªè c√¢u list (nhi·ªÅu d·∫•u ph·∫©y, bullet points)
        if sentence.text.strip().startswith(('-', '‚Ä¢', '*', '+')):
            continue
        
        valid_sentences.append(sentence)
    
    return valid_sentences


# ============================================================================
# B∆Ø·ªöC 2.4 ‚Äì CH·∫§M ƒêI·ªÇM C√ÇU (CORE LOGIC)
# ============================================================================

def calculate_keyword_density(
    sentence: Sentence,
    vocabulary_words: List[str]
) -> float:
    """
    T√≠nh keyword density: t·ª∑ l·ªá t·ª´ quan tr·ªçng trong c√¢u
    
    Args:
        sentence: C√¢u c·∫ßn t√≠nh
        vocabulary_words: Danh s√°ch t·ª´ v·ª±ng
    
    Returns:
        Keyword density (0-1)
    """
    text_lower = sentence.text.lower()
    keyword_count = 0
    
    for word in vocabulary_words:
        word_pattern = re.compile(r'\b' + re.escape(word.lower()) + r'\b')
        matches = word_pattern.findall(text_lower)
        keyword_count += len(matches)
    
    return keyword_count / sentence.word_count if sentence.word_count > 0 else 0


def calculate_length_score(
    sentence: Sentence,
    optimal_min: int = 8,
    optimal_max: int = 20
) -> float:
    """
    T√≠nh length score: c√¢u 8-20 t·ª´ l√† t·ªët nh·∫•t
    
    Args:
        sentence: C√¢u c·∫ßn t√≠nh
        optimal_min: S·ªë t·ª´ t·ªëi thi·ªÉu l√Ω t∆∞·ªüng
        optimal_max: S·ªë t·ª´ t·ªëi ƒëa l√Ω t∆∞·ªüng
    
    Returns:
        Length score (0-1)
    """
    word_count = sentence.word_count
    
    if optimal_min <= word_count <= optimal_max:
        return 1.0  # Perfect length
    
    if word_count < optimal_min:
        # Too short: linear penalty
        return word_count / optimal_min
    
    # Too long: exponential penalty
    excess = word_count - optimal_max
    return math.exp(-excess / 10)


def calculate_position_score(
    sentence: Sentence,
    total_sentences: int
) -> float:
    """
    T√≠nh position score: c√¢u xu·∫•t hi·ªán s·ªõm h∆°n ‚Üí quan tr·ªçng h∆°n
    
    Args:
        sentence: C√¢u c·∫ßn t√≠nh
        total_sentences: T·ªïng s·ªë c√¢u
    
    Returns:
        Position score (0-1)
    """
    # Exponential decay: c√¢u ƒë·∫ßu ti√™n = 1.0, gi·∫£m d·∫ßn
    return math.exp(-sentence.position / (total_sentences * 0.3))


def calculate_clarity_score(sentence: Sentence) -> float:
    """
    T√≠nh clarity score: c√≥ ƒë·ªông t·ª´, kh√¥ng ph·∫£i list
    
    Args:
        sentence: C√¢u c·∫ßn t√≠nh
    
    Returns:
        Clarity score (0-1)
    """
    score = 0.0
    
    # C√≥ ƒë·ªông t·ª´: +0.5
    if sentence.has_verb:
        score += 0.5
    
    # Kh√¥ng ph·∫£i list (√≠t d·∫•u ph·∫©y)
    comma_count = sentence.text.count(',')
    comma_ratio = comma_count / sentence.word_count if sentence.word_count > 0 else 0
    
    if comma_ratio < 0.15:
        score += 0.3  # C√¢u t·ª± nhi√™n
    elif comma_ratio < 0.3:
        score += 0.15  # C√¢u h∆°i nhi·ªÅu ph·∫©y
    
    # Kh√¥ng c√≥ bullet points ho·∫∑c numbering
    if not re.match(r'^[\d\-‚Ä¢*+]', sentence.text.strip()):
        score += 0.2
    
    return min(score, 1.0)


def score_sentence(
    sentence: Sentence,
    vocabulary_words: List[str],
    total_sentences: int,
    weights: Optional[Dict[str, float]] = None
) -> SentenceScore:
    """
    Ch·∫•m ƒëi·ªÉm m·ªôt c√¢u theo c√¥ng th·ª©c weighted
    
    Formula:
    score = 0.4 * keyword_density + 0.3 * length_score + 
            0.2 * position_score + 0.1 * clarity_score
    
    Args:
        sentence: C√¢u c·∫ßn ch·∫•m ƒëi·ªÉm
        vocabulary_words: Danh s√°ch t·ª´ v·ª±ng
        total_sentences: T·ªïng s·ªë c√¢u
        weights: Tr·ªçng s·ªë t√πy ch·ªânh
    
    Returns:
        SentenceScore object
    """
    if weights is None:
        weights = {
            'keyword_density': 0.4,
            'length_score': 0.3,
            'position_score': 0.2,
            'clarity_score': 0.1
        }
    
    # T√≠nh c√°c ƒëi·ªÉm th√†nh ph·∫ßn
    keyword_density = calculate_keyword_density(sentence, vocabulary_words)
    length_score = calculate_length_score(sentence)
    position_score = calculate_position_score(sentence, total_sentences)
    clarity_score = calculate_clarity_score(sentence)
    
    # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p
    final_score = (
        weights['keyword_density'] * keyword_density +
        weights['length_score'] * length_score +
        weights['position_score'] * position_score +
        weights['clarity_score'] * clarity_score
    )
    
    return SentenceScore(
        sentence_id=sentence.sentence_id,
        score=final_score,
        breakdown={
            'keyword_density': keyword_density,
            'length_score': length_score,
            'position_score': position_score,
            'clarity_score': clarity_score
        }
    )


# ============================================================================
# B∆Ø·ªöC 2.5 ‚Äì CH·ªåN C√ÇU T·ªêT NH·∫§T & HIGHLIGHT
# ============================================================================

def highlight_word(sentence_text: str, word: str) -> str:
    """
    Highlight t·ª´ v·ª±ng trong c√¢u b·∫±ng HTML <b> tags
    
    Args:
        sentence_text: C√¢u g·ªëc
        word: T·ª´ c·∫ßn highlight
    
    Returns:
        C√¢u v·ªõi t·ª´ ƒë∆∞·ª£c highlight
    """
    word_pattern = re.compile(r'\b(' + re.escape(word) + r')\b', re.IGNORECASE)
    return word_pattern.sub(r'<b>\1</b>', sentence_text)


def generate_explanation(
    sentence_score: SentenceScore,
    sentence: Sentence
) -> str:
    """
    T·∫°o explanation cho vi·ªác ch·ªçn c√¢u
    
    Args:
        sentence_score: ƒêi·ªÉm s·ªë c·ªßa c√¢u
        sentence: C√¢u ƒë∆∞·ª£c ch·ªçn
    
    Returns:
        Explanation string
    """
    breakdown = sentence_score.breakdown
    reasons = []
    
    if breakdown['keyword_density'] > 0.15:
        reasons.append(f"m·∫≠t ƒë·ªô t·ª´ kh√≥a cao ({breakdown['keyword_density']*100:.1f}%)")
    
    if breakdown['length_score'] > 0.8:
        reasons.append(f"ƒë·ªô d√†i t·ªëi ∆∞u ({sentence.word_count} t·ª´)")
    
    if breakdown['position_score'] > 0.7:
        reasons.append("xu·∫•t hi·ªán s·ªõm trong t√†i li·ªáu")
    
    if breakdown['clarity_score'] > 0.7:
        reasons.append("c√¢u r√µ r√†ng, c√≥ ƒë·ªông t·ª´")
    
    if not reasons:
        reasons.append("ƒëi·ªÉm t·ªïng h·ª£p cao")
    
    return f"ƒê∆∞·ª£c ch·ªçn v√¨: {', '.join(reasons)}. Score: {sentence_score.score:.3f}"


def select_best_contexts(
    vocabulary_list: List[Dict[str, float]],
    sentences: List[Sentence],
    min_words: int = 5,
    max_words: int = 35,
    require_verb: bool = True,
    weights: Optional[Dict[str, float]] = None
) -> List[VocabularyContext]:
    """
    Ch·ªçn c√¢u t·ªët nh·∫•t cho m·ªói t·ª´ v·ª±ng
    
    Args:
        vocabulary_list: Danh s√°ch t·ª´ v·ª±ng v·ªõi ƒëi·ªÉm
        sentences: Danh s√°ch c√¢u
        min_words: S·ªë t·ª´ t·ªëi thi·ªÉu
        max_words: S·ªë t·ª´ t·ªëi ƒëa
        require_verb: Y√™u c·∫ßu c√≥ ƒë·ªông t·ª´
        weights: Tr·ªçng s·ªë t√πy ch·ªânh
    
    Returns:
        Danh s√°ch VocabularyContext
    """
    # B∆∞·ªõc 1: L·ªçc c√¢u h·ª£p l·ªá
    valid_sentences = filter_invalid_sentences(
        sentences, min_words, max_words, require_verb
    )
    
    if not valid_sentences:
        print("‚ö†Ô∏è  No valid sentences found")
        return []
    
    # B∆∞·ªõc 2: Map t·ª´ ‚Üí c√¢u
    vocabulary_words = [v['word'] for v in vocabulary_list]
    word_map = map_words_to_sentences(vocabulary_words, valid_sentences)
    
    # B∆∞·ªõc 3: Ch·ªçn c√¢u t·ªët nh·∫•t cho m·ªói t·ª´
    contexts = []
    
    for vocab_item in vocabulary_list:
        word = vocab_item['word']
        sentence_ids = word_map.get(word, [])
        
        if not sentence_ids:
            # Only warn for single words (not phrases)
            if ' ' not in word and len(word) <= 20:
                print(f"‚ö†Ô∏è  No sentences found for word: {word}")
            continue
        
        # L·∫•y c√°c c√¢u ch·ª©a t·ª´ n√†y
        candidate_sentences = [s for s in valid_sentences if s.sentence_id in sentence_ids]
        
        # Ch·∫•m ƒëi·ªÉm c√°c c√¢u
        sentence_scores = [
            score_sentence(s, vocabulary_words, len(valid_sentences), weights)
            for s in candidate_sentences
        ]
        
        # Ch·ªçn c√¢u c√≥ ƒëi·ªÉm cao nh·∫•t
        best_score = max(sentence_scores, key=lambda x: x.score)
        best_sentence = next(s for s in candidate_sentences if s.sentence_id == best_score.sentence_id)
        
        # Highlight t·ª´ v·ª±ng trong c√¢u
        highlighted_sentence = highlight_word(best_sentence.text, word)
        
        # T·∫°o explanation
        explanation = generate_explanation(best_score, best_sentence)
        
        context = VocabularyContext(
            word=word,
            final_score=vocab_item['score'],
            context_sentence=highlighted_sentence,
            sentence_id=best_score.sentence_id,
            sentence_score=best_score.score,
            explanation=explanation
        )
        
        contexts.append(context)
    
    return contexts


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def select_vocabulary_contexts(
    text: str,
    vocabulary_list: List[Dict[str, float]],
    language: str = "en",
    min_words: int = 5,
    max_words: int = 35,
    optimal_min: int = 8,
    optimal_max: int = 20,
    require_verb: bool = True,
    weights: Optional[Dict[str, float]] = None
) -> List[Dict]:
    """
    STAGE 2 Main Function: Ch·ªçn ng·ªØ c·∫£nh t·ªët nh·∫•t cho t·ª´ v·ª±ng
    
    Args:
        text: VƒÉn b·∫£n ƒë√£ l√†m s·∫°ch
        vocabulary_list: Danh s√°ch t·ª´ v·ª±ng v·ªõi ƒëi·ªÉm [{'word': str, 'score': float}]
        language: Ng√¥n ng·ªØ ('en' ho·∫∑c 'vi')
        min_words: S·ªë t·ª´ t·ªëi thi·ªÉu trong c√¢u
        max_words: S·ªë t·ª´ t·ªëi ƒëa trong c√¢u
        optimal_min: S·ªë t·ª´ t·ªëi thi·ªÉu l√Ω t∆∞·ªüng
        optimal_max: S·ªë t·ª´ t·ªëi ƒëa l√Ω t∆∞·ªüng
        require_verb: Y√™u c·∫ßu c√≥ ƒë·ªông t·ª´
        weights: Tr·ªçng s·ªë t√πy ch·ªânh
    
    Returns:
        Danh s√°ch context d·∫°ng dictionary
    """
    print(f"[Context Selector] Starting context selection...")
    print(f"[Context Selector] Input: {len(vocabulary_list)} vocabulary words")
    
    # B∆∞·ªõc 1: Build sentences
    sentences = build_sentences(text, language)
    print(f"[Context Selector] Built {len(sentences)} sentences")
    
    # B∆∞·ªõc 2-5: Select best contexts
    contexts = select_best_contexts(
        vocabulary_list,
        sentences,
        min_words,
        max_words,
        require_verb,
        weights
    )
    print(f"[Context Selector] Selected {len(contexts)} contexts")
    
    # Convert to dictionary
    return [
        {
            'word': ctx.word,
            'finalScore': ctx.final_score,
            'contextSentence': ctx.context_sentence,
            'sentenceId': ctx.sentence_id,
            'sentenceScore': ctx.sentence_score,
            'explanation': ctx.explanation
        }
        for ctx in contexts
    ]


# ============================================================================
# TESTING
# ============================================================================

if __name__ == "__main__":
    # Test v·ªõi vƒÉn b·∫£n m·∫´u
    test_text = """
    Ontology is a formal representation of knowledge within a domain.
    It defines concepts, properties, and relationships between entities.
    Knowledge graphs are built using ontologies.
    Semantic web technologies rely heavily on ontology engineering.
    The Web Ontology Language (OWL) is a standard for creating ontologies.
    """
    
    test_vocabulary = [
        {'word': 'ontology', 'score': 0.95},
        {'word': 'knowledge', 'score': 0.85},
        {'word': 'semantic', 'score': 0.80}
    ]
    
    print("=" * 80)
    print("TESTING STAGE 2 - Context Intelligence Engine")
    print("=" * 80)
    
    contexts = select_vocabulary_contexts(test_text, test_vocabulary)
    
    print("\nüìä RESULTS:")
    print("-" * 80)
    for ctx in contexts:
        print(f"\nüîπ Word: {ctx['word']}")
        print(f"   Context: {ctx['contextSentence']}")
        print(f"   Score: {ctx['sentenceScore']:.3f}")
        print(f"   {ctx['explanation']}")
    
    print("\n‚úÖ Test completed!")
