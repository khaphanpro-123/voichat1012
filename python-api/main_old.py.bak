"""
Visual Language Tutor - Backend API
T√≠ch h·ª£p YOLO Custom + OCR + GPT-4o + Context Intelligence Engine
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import shutil
import os
import json
from datetime import datetime
import hashlib

# Import AI modules
from ultralytics import YOLO
import easyocr
from openai import OpenAI
from PIL import Image
import numpy as np

# Import custom modules
from ensemble_extractor import extract_vocabulary_ensemble
from context_intelligence import select_vocabulary_contexts

# ==================== CONFIGURATION ====================

app = FastAPI(title="Visual Language Tutor API", version="1.0.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Directories
os.makedirs("uploads", exist_ok=True)
os.makedirs("cache", exist_ok=True)
os.makedirs("models", exist_ok=True)

# AI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
YOLO_MODEL_PATH = "models/custom_yolo.pt"  # Model custom c·ªßa b·∫°n
CONFIDENCE_THRESHOLD = 0.4

# Initialize AI Models
print("üîÑ Loading AI models...")

try:
    yolo_model = YOLO(YOLO_MODEL_PATH)
    print("‚úÖ YOLO Custom model loaded")
except:
    # Fallback to standard YOLO if custom not available
    yolo_model = YOLO("yolov8n.pt")
    print("‚ö†Ô∏è  Using standard YOLOv8n (replace with custom model)")

ocr_reader = easyocr.Reader(['en', 'vi'], gpu=False)
print("‚úÖ OCR model loaded")

openai_client = OpenAI(api_key=OPENAI_API_KEY)
print("‚úÖ OpenAI client initialized")


# ==================== DATA MODELS ====================

class VocabularyItem(BaseModel):
    word: str
    ipa: str
    meaning_vi: str
    part_of_speech: str
    example_sentence: str
    confidence: float = 1.0


class AnalysisResponse(BaseModel):
    image_id: str
    timestamp: str
    detections: dict
    vocabulary: List[dict]
    sentences: List[str]
    ocr_texts: List[str]
    cache_hit: bool


# ==================== HELPER FUNCTIONS ====================

def generate_image_hash(image_path: str) -> str:
    """Generate unique hash for image caching"""
    with open(image_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()


def check_cache(image_hash: str) -> Optional[dict]:
    """Check if analysis exists in cache"""
    cache_file = f"cache/{image_hash}.json"
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def save_cache(image_hash: str, data: dict):
    """Save analysis to cache"""
    cache_file = f"cache/{image_hash}.json"
    with open(cache_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ==================== AI PROCESSING ====================

def detect_with_yolo(image_path: str) -> dict:
    """Nh·∫≠n d·∫°ng v·∫≠t th·ªÉ v·ªõi YOLO"""
    results = yolo_model(image_path, conf=CONFIDENCE_THRESHOLD)
    
    detections = {
        "objects": [],
        "total_count": 0
    }
    
    for r in results:
        for box in r.boxes:
            label = yolo_model.names[int(box.cls)]
            confidence = float(box.conf)
            x1, y1, x2, y2 = box.xyxy[0].tolist()
            
            detection = {
                "label": label,
                "label_vi": translate_label(label),
                "confidence": round(confidence, 3),
                "bbox": [int(x1), int(y1), int(x2), int(y2)]
            }
            detections["objects"].append(detection)
    
    detections["total_count"] = len(detections["objects"])
    return detections


def translate_label(label: str) -> str:
    """D·ªãch label sang ti·∫øng Vi·ªát (basic)"""
    translations = {
        "person": "ng∆∞·ªùi",
        "car": "xe h∆°i",
        "dog": "con ch√≥",
        "cat": "con m√®o",
        "chair": "gh·∫ø",
        "table": "b√†n",
        "book": "s√°ch",
        "phone": "ƒëi·ªán tho·∫°i",
        "laptop": "m√°y t√≠nh x√°ch tay",
        "bottle": "chai",
        "cup": "c·ªëc",
        "tv": "tivi",
        "bed": "gi∆∞·ªùng",
        "clock": "ƒë·ªìng h·ªì",
        "keyboard": "b√†n ph√≠m",
        "mouse": "chu·ªôt m√°y t√≠nh",
        "backpack": "ba l√¥",
        "umbrella": "√¥/d√π",
        "handbag": "t√∫i x√°ch",
        "bicycle": "xe ƒë·∫°p",
        "motorcycle": "xe m√°y",
        "bus": "xe bu√Ωt",
        "train": "t√†u h·ªèa",
        "airplane": "m√°y bay",
        "bird": "chim",
        "horse": "ng·ª±a",
        "cow": "b√≤",
        "sheep": "c·ª´u",
        "elephant": "voi",
        "bear": "g·∫•u",
        "zebra": "ng·ª±a v·∫±n",
        "giraffe": "h∆∞∆°u cao c·ªï",
    }
    return translations.get(label.lower(), label)


def extract_text_ocr(image_path: str) -> List[str]:
    """OCR to√†n b·ªô ·∫£nh"""
    try:
        results = ocr_reader.readtext(image_path, detail=0)
        # Filter meaningful text
        texts = [t.strip() for t in results if len(t.strip()) > 2 and any(c.isalpha() for c in t)]
        return texts
    except Exception as e:
        print(f"OCR Error: {e}")
        return []


def generate_vocabulary_with_gpt(detections: dict, ocr_texts: List[str]) -> dict:
    """G·ªçi GPT-4o ƒë·ªÉ sinh t·ª´ v·ª±ng"""
    
    object_labels = [d["label"] for d in detections["objects"]]
    
    if not object_labels and not ocr_texts:
        return {
            "vocabulary": [],
            "sentences": [],
            "summary": "No content detected in image."
        }
    
    prompt = f"""You are an English tutor for Vietnamese A2-B1 learners.

DETECTED IN IMAGE:
- Objects: {object_labels[:10]}
- Text (OCR): {ocr_texts[:10]}

TASKS:
1. Extract 5-8 useful English vocabulary words
2. For each word provide:
   - IPA pronunciation
   - Vietnamese meaning
   - Part of speech (noun/verb/adj/adv)
   - One simple example sentence (A2 level)
3. Create 2-3 simple sentences about the image

RESPONSE FORMAT (JSON only):
{{
  "vocabulary": [
    {{
      "word": "education",
      "ipa": "/Àåed íuÀàke…™ Én/",
      "meaning_vi": "gi√°o d·ª•c",
      "part_of_speech": "noun",
      "example_sentence": "Education is important."
    }}
  ],
  "sentences": [
    "I can see a person in the image.",
    "There is a book on the table."
  ]
}}

Output valid JSON only. A2 level vocabulary."""

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful English tutor. Always respond with valid JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=1500
        )
        
        content = response.choices[0].message.content
        
        # Remove markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        
        result = json.loads(content.strip())
        return result
        
    except Exception as e:
        print(f"GPT Error: {e}")
        return {
            "vocabulary": [],
            "sentences": [],
            "error": str(e)
        }


# ==================== API ENDPOINTS ====================

@app.get("/")
def root():
    return {
        "message": "Visual Language Tutor API",
        "version": "1.0.0",
        "status": "running",
        "models": {
            "yolo": "loaded",
            "ocr": "loaded",
            "gpt": "connected"
        }
    }


@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "yolo_model": YOLO_MODEL_PATH,
        "ocr_languages": ["en", "vi"]
    }


@app.post("/api/analyze-image")
async def analyze_image(file: UploadFile = File(...)):
    """Main endpoint: Ph√¢n t√≠ch ·∫£nh v√† sinh t·ª´ v·ª±ng"""
    
    try:
        # Save uploaded file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{file.filename}"
        image_path = f"uploads/{filename}"
        
        with open(image_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Generate hash for caching
        image_hash = generate_image_hash(image_path)
        
        # Check cache
        cached_result = check_cache(image_hash)
        if cached_result:
            print(f"‚úÖ Cache hit: {image_hash}")
            cached_result["cache_hit"] = True
            return JSONResponse(content=cached_result)
        
        print(f"üîç Processing new image: {filename}")
        
        # Step 1: YOLO Detection
        detections = detect_with_yolo(image_path)
        print(f"   Found {detections['total_count']} objects")
        
        # Step 2: OCR
        ocr_texts = extract_text_ocr(image_path)
        print(f"   OCR found {len(ocr_texts)} text segments")
        
        # Step 3: GPT sinh t·ª´ v·ª±ng
        learning_content = generate_vocabulary_with_gpt(detections, ocr_texts)
        
        # Prepare response
        result = {
            "success": True,
            "image_id": image_hash,
            "timestamp": datetime.now().isoformat(),
            "detections": detections,
            "ocr_texts": ocr_texts,
            "vocabulary": learning_content.get("vocabulary", []),
            "sentences": learning_content.get("sentences", []),
            "cache_hit": False
        }
        
        # Save to cache
        save_cache(image_hash, result)
        
        return JSONResponse(content=result)
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== RUN SERVER ====================

if __name__ == "__main__":
    import uvicorn
    print("üöÄ Starting Visual Language Tutor API on port 8000...")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)


# ==================== NEW ENDPOINTS: SMART VOCABULARY EXTRACTION ====================

class SmartVocabularyRequest(BaseModel):
    text: str
    max_words: int = 50
    language: str = "en"


@app.post("/api/smart-vocabulary-extract")
async def smart_vocabulary_extract(request: SmartVocabularyRequest):
    """
    Smart Vocabulary Extraction with Context Intelligence
    
    Pipeline:
    - STAGE 1: Extract vocabulary using ensemble methods (TF-IDF + RAKE + YAKE)
    - STAGE 2: Select best context sentences for each word
    - STAGE 3: Return vocabulary with highlighted context
    """
    try:
        text = request.text
        max_words = request.max_words
        language = request.language
        
        if not text or len(text) < 50:
            raise HTTPException(
                status_code=400,
                detail="Text qu√° ng·∫Øn (c·∫ßn √≠t nh·∫•t 50 k√Ω t·ª±)"
            )
        
        print(f"[Smart Vocabulary] Starting extraction...")
        print(f"[Smart Vocabulary] Input text length: {len(text)} chars")
        
        # ====================================================================
        # STAGE 1: Extract Vocabulary using Ensemble Methods
        # ====================================================================
        print("[STAGE 1] Extracting vocabulary with ensemble methods...")
        
        ensemble_result = extract_vocabulary_ensemble(
            text,
            max_words=max_words,
            min_word_length=3,
            weights={
                'frequency': 0.15,
                'tfidf': 0.35,
                'rake': 0.25,
                'yake': 0.25
            },
            include_ngrams=True,
            filter_proper_nouns=True,
            filter_technical=True,
            context_filtering=True
        )
        
        print(f"[STAGE 1] Extracted {len(ensemble_result['vocabulary'])} vocabulary words")
        
        # Prepare vocabulary list with scores
        vocabulary_list = [
            {
                'word': score['word'],
                'score': score['score']
            }
            for score in ensemble_result['scores']
        ]
        
        # ====================================================================
        # STAGE 2: Select Best Context Sentences
        # ====================================================================
        print("[STAGE 2] Selecting best context sentences...")
        
        contexts = select_vocabulary_contexts(
            text,
            vocabulary_list,
            language=language,
            min_words=5,
            max_words=35,
            optimal_min=8,
            optimal_max=20,
            require_verb=True,
            weights={
                'keyword_density': 0.4,
                'length_score': 0.3,
                'position_score': 0.2,
                'clarity_score': 0.1
            }
        )
        
        print(f"[STAGE 2] Selected {len(contexts)} contexts")
        
        # ====================================================================
        # STAGE 3: Combine Results
        # ====================================================================
        results = []
        for ctx in contexts:
            # Find original score data
            score_data = next(
                (s for s in ensemble_result['scores'] if s['word'] == ctx['word']),
                None
            )
            
            result = {
                'word': ctx['word'],
                'score': ctx['finalScore'],
                'context': ctx['contextSentence'],  # Highlighted
                'contextPlain': ctx['contextSentence'].replace('<b>', '').replace('</b>', ''),
                'sentenceId': ctx['sentenceId'],
                'sentenceScore': ctx['sentenceScore'],
                'explanation': ctx['explanation'],
                'features': score_data['features'] if score_data else {}
            }
            results.append(result)
        
        # ====================================================================
        # Return Results
        # ====================================================================
        return JSONResponse(content={
            'success': True,
            'vocabulary': results,
            'count': len(results),
            'stats': {
                'stage1': ensemble_result['stats'],
                'stage2': {
                    'totalContexts': len(contexts),
                    'avgSentenceScore': sum(c['sentenceScore'] for c in contexts) / len(contexts) if contexts else 0
                }
            },
            'pipeline': 'STAGE 1 (Ensemble) + STAGE 2 (Context Intelligence)'
        })
        
    except Exception as e:
        print(f"[Smart Vocabulary] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/smart-vocabulary-extract")
async def smart_vocabulary_info():
    """API information"""
    return {
        'success': True,
        'message': 'Smart Vocabulary Extraction API with Context Intelligence',
        'pipeline': [
            'STAGE 1: Ensemble Vocabulary Extraction (TF-IDF + RAKE + YAKE)',
            'STAGE 2: Context Intelligence Engine (Sentence Scoring & Selection)',
            'STAGE 3: Flashcard Generation with Context'
        ],
        'features': [
            '‚úÖ Rule-based, deterministic output',
            '‚úÖ Explainable scoring logic',
            '‚úÖ Context-aware sentence selection',
            '‚úÖ Automatic word highlighting',
            '‚úÖ Quality filtering (proper nouns, technical terms)'
        ],
        'usage': {
            'method': 'POST',
            'endpoint': '/api/smart-vocabulary-extract',
            'body': {
                'text': 'Document text to extract vocabulary from',
                'max_words': 'Maximum number of words to extract (default: 50)',
                'language': 'Language code: en or vi (default: en)'
            }
        }
    }


# ==================== STAGE 3: LEARNING FEEDBACK LOOP ====================

from feedback_loop import FeedbackLoop, FeedbackMemory

# Initialize feedback loop
feedback_loop = FeedbackLoop(storage_path="feedback_data", learning_rate=0.1)

class FeedbackRequest(BaseModel):
    word: str
    document_id: str
    user_id: str
    scores: Dict[str, float]  # {tfidf, frequency, yake, rake}
    final_score: float
    user_action: str  # "keep", "drop", "star"


@app.post("/api/vocabulary-feedback")
async def submit_vocabulary_feedback(request: FeedbackRequest):
    """
    Submit user feedback for vocabulary word
    
    This endpoint collects user feedback and adaptively adjusts
    ensemble weights for future extractions (pseudo-training).
    """
    try:
        # Validate user_action
        if request.user_action not in ['keep', 'drop', 'star']:
            raise HTTPException(
                status_code=400,
                detail="user_action must be 'keep', 'drop', or 'star'"
            )
        
        # Process feedback
        result = feedback_loop.process_feedback(
            word=request.word,
            document_id=request.document_id,
            user_id=request.user_id,
            scores=request.scores,
            final_score=request.final_score,
            user_action=request.user_action
        )
        
        return JSONResponse(content={
            'success': True,
            'feedback_saved': result['feedback_saved'],
            'weights_updated': result['weights_updated'],
            'new_weights': result['new_weights'],
            'explanation': result['explanation']
        })
        
    except Exception as e:
        print(f"[Feedback] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/vocabulary-feedback/statistics")
async def get_feedback_statistics(user_id: Optional[str] = None):
    """
    Get feedback statistics and current ensemble weights
    
    Query params:
    - user_id: Optional, filter by specific user
    """
    try:
        if user_id:
            memory = FeedbackMemory()
            user_feedbacks = memory.get_feedback_by_user(user_id)
            
            stats = {
                'total': len(user_feedbacks),
                'keep': sum(1 for fb in user_feedbacks if fb.user_action == 'keep'),
                'drop': sum(1 for fb in user_feedbacks if fb.user_action == 'drop'),
                'star': sum(1 for fb in user_feedbacks if fb.user_action == 'star')
            }
        else:
            stats = feedback_loop.get_statistics()
        
        return JSONResponse(content={
            'success': True,
            'statistics': stats
        })
        
    except Exception as e:
        print(f"[Feedback Stats] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/vocabulary-feedback/weights")
async def get_current_weights():
    """Get current ensemble weights"""
    try:
        weights = feedback_loop.get_current_weights()
        stats = feedback_loop.get_statistics()
        
        return JSONResponse(content={
            'success': True,
            'weights': weights,
            'version': stats.get('weights_version', 0),
            'last_updated': stats.get('last_updated', ''),
            'feedback_count': stats.get('feedback_stats', {}).get('total', 0)
        })
        
    except Exception as e:
        print(f"[Weights] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/smart-vocabulary-extract-adaptive")
async def smart_vocabulary_extract_adaptive(request: SmartVocabularyRequest):
    """
    Smart Vocabulary Extraction with Adaptive Weights
    
    This endpoint uses the current adaptive weights from feedback loop
    instead of fixed weights.
    """
    try:
        text = request.text
        max_words = request.max_words
        language = request.language
        
        if not text or len(text) < 50:
            raise HTTPException(
                status_code=400,
                detail="Text qu√° ng·∫Øn (c·∫ßn √≠t nh·∫•t 50 k√Ω t·ª±)"
            )
        
        print(f"[Adaptive Extract] Starting with adaptive weights...")
        
        # Get current adaptive weights
        adaptive_weights = feedback_loop.get_current_weights()
        print(f"[Adaptive Extract] Using weights: {adaptive_weights}")
        
        # STAGE 1: Extract with adaptive weights
        ensemble_result = extract_vocabulary_ensemble(
            text,
            max_words=max_words,
            min_word_length=3,
            weights=adaptive_weights,  # ‚úÖ Use adaptive weights
            include_ngrams=True,
            filter_proper_nouns=True,
            filter_technical=True,
            context_filtering=True
        )
        
        vocabulary_list = [
            {'word': score['word'], 'score': score['score']}
            for score in ensemble_result['scores']
        ]
        
        # STAGE 2: Select contexts
        contexts = select_vocabulary_contexts(
            text,
            vocabulary_list,
            language=language
        )
        
        # Prepare results
        results = []
        for ctx in contexts:
            score_data = next(
                (s for s in ensemble_result['scores'] if s['word'] == ctx['word']),
                None
            )
            
            result = {
                'word': ctx['word'],
                'score': ctx['finalScore'],
                'context': ctx['contextSentence'],
                'contextPlain': ctx['contextSentence'].replace('<b>', '').replace('</b>', ''),
                'sentenceId': ctx['sentenceId'],
                'sentenceScore': ctx['sentenceScore'],
                'explanation': ctx['explanation'],
                'features': score_data['features'] if score_data else {}
            }
            results.append(result)
        
        # Get weights info
        weights_info = feedback_loop.get_statistics()
        
        return JSONResponse(content={
            'success': True,
            'vocabulary': results,
            'count': len(results),
            'stats': {
                'stage1': ensemble_result['stats'],
                'stage2': {
                    'totalContexts': len(contexts),
                    'avgSentenceScore': sum(c['sentenceScore'] for c in contexts) / len(contexts) if contexts else 0
                }
            },
            'adaptive_weights': {
                'weights': adaptive_weights,
                'version': weights_info.get('weights_version', 0),
                'feedback_count': weights_info.get('feedback_stats', {}).get('total', 0)
            },
            'pipeline': 'STAGE 1 (Adaptive Ensemble) + STAGE 2 (Context Intelligence) + STAGE 3 (Feedback Loop)'
        })
        
    except Exception as e:
        print(f"[Adaptive Extract] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== STAGE 4: ONTOLOGY & KNOWLEDGE GRAPH ====================

from knowledge_graph import (
    KnowledgeGraph,
    EntityType,
    RelationType
)

# Initialize Knowledge Graph
knowledge_graph = KnowledgeGraph(storage_path="knowledge_graph_data")

class KnowledgeGraphRequest(BaseModel):
    document_id: str
    document_title: str
    document_content: str
    vocabulary_contexts: List[Dict]


@app.post("/api/knowledge-graph/build")
async def build_knowledge_graph(request: KnowledgeGraphRequest):
    """
    Build Knowledge Graph from vocabulary contexts
    
    Transforms flat data from STAGE 2 into structured ontology-based graph
    """
    try:
        stats = knowledge_graph.build_from_vocabulary_contexts(
            vocabulary_contexts=request.vocabulary_contexts,
            document_id=request.document_id,
            document_title=request.document_title,
            document_content=request.document_content
        )
        
        # Save graph
        knowledge_graph.save_graph()
        
        # Get overall statistics
        graph_stats = knowledge_graph.get_statistics()
        
        return JSONResponse(content={
            'success': True,
            'build_stats': stats,
            'graph_stats': graph_stats,
            'message': f"Knowledge graph built for document: {request.document_id}"
        })
        
    except Exception as e:
        print(f"[KG Build] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/query/vocabulary/{document_id}")
async def query_vocabulary_by_document(document_id: str):
    """Query all vocabulary terms in a document"""
    try:
        vocab_terms = knowledge_graph.query_vocabulary_by_document(document_id)
        
        results = [
            {
                'term_id': term.entity_id,
                'word': term.properties.get('word'),
                'score': term.properties.get('score'),
                'context': term.properties.get('context_sentence')
            }
            for term in vocab_terms
        ]
        
        return JSONResponse(content={
            'success': True,
            'document_id': document_id,
            'vocabulary_count': len(results),
            'vocabulary': results
        })
        
    except Exception as e:
        print(f"[KG Query] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/query/context/{term_id}")
async def query_context_for_term(term_id: str):
    """Query context sentence for a vocabulary term"""
    try:
        context = knowledge_graph.query_context_for_term(term_id)
        
        if not context:
            return JSONResponse(content={
                'success': False,
                'message': f"No context found for term: {term_id}"
            })
        
        return JSONResponse(content={
            'success': True,
            'term_id': term_id,
            'context': {
                'sentence_id': context.entity_id,
                'text': context.properties.get('text'),
                'position': context.properties.get('position')
            }
        })
        
    except Exception as e:
        print(f"[KG Query] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/query/related/{term_id}")
async def query_related_terms(term_id: str, max_depth: int = 2):
    """Query related vocabulary terms"""
    try:
        related_terms = knowledge_graph.query_related_terms(term_id, max_depth)
        
        results = [
            {
                'term_id': term.entity_id,
                'word': term.properties.get('word'),
                'score': term.properties.get('score')
            }
            for term in related_terms
        ]
        
        return JSONResponse(content={
            'success': True,
            'term_id': term_id,
            'related_count': len(results),
            'related_terms': results
        })
        
    except Exception as e:
        print(f"[KG Query] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/statistics")
async def get_knowledge_graph_statistics():
    """Get Knowledge Graph statistics"""
    try:
        stats = knowledge_graph.get_statistics()
        
        return JSONResponse(content={
            'success': True,
            'statistics': stats
        })
        
    except Exception as e:
        print(f"[KG Stats] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/export/cypher")
async def export_knowledge_graph_to_cypher():
    """Export Knowledge Graph to Cypher queries (for Neo4j)"""
    try:
        filepath = knowledge_graph.export_to_cypher()
        
        return JSONResponse(content={
            'success': True,
            'message': 'Graph exported to Cypher format',
            'filepath': filepath
        })
        
    except Exception as e:
        print(f"[KG Export] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/complete-pipeline")
async def complete_pipeline(request: SmartVocabularyRequest):
    """
    Complete Pipeline: STAGE 1 ‚Üí 2 ‚Üí 3 ‚Üí 4
    
    Extract vocabulary, select contexts, apply feedback, build knowledge graph
    """
    try:
        text = request.text
        max_words = request.max_words
        language = request.language
        
        print("[Complete Pipeline] Starting...")
        
        # STAGE 1: Ensemble Extraction (with adaptive weights)
        adaptive_weights = feedback_loop.get_current_weights()
        ensemble_result = extract_vocabulary_ensemble(
            text,
            max_words=max_words,
            weights=adaptive_weights
        )
        
        vocabulary_list = [
            {'word': score['word'], 'score': score['score']}
            for score in ensemble_result['scores']
        ]
        
        # STAGE 2: Context Selection
        contexts = select_vocabulary_contexts(text, vocabulary_list, language)
        
        # Prepare results
        vocabulary_contexts = []
        for ctx in contexts:
            score_data = next(
                (s for s in ensemble_result['scores'] if s['word'] == ctx['word']),
                None
            )
            
            vocabulary_contexts.append({
                'word': ctx['word'],
                'finalScore': ctx['finalScore'],
                'contextSentence': ctx['contextSentence'],
                'sentenceId': ctx['sentenceId'],
                'sentenceScore': ctx['sentenceScore'],
                'explanation': ctx['explanation'],
                'features': score_data['features'] if score_data else {}
            })
        
        # STAGE 4: Build Knowledge Graph
        document_id = f"doc_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        kg_stats = knowledge_graph.build_from_vocabulary_contexts(
            vocabulary_contexts=vocabulary_contexts,
            document_id=document_id,
            document_title="Extracted Document",
            document_content=text[:500]  # First 500 chars
        )
        
        knowledge_graph.save_graph()
        
        return JSONResponse(content={
            'success': True,
            'document_id': document_id,
            'vocabulary': vocabulary_contexts,
            'count': len(vocabulary_contexts),
            'pipeline_stats': {
                'stage1': ensemble_result['stats'],
                'stage2': {
                    'totalContexts': len(contexts),
                    'avgSentenceScore': sum(c['sentenceScore'] for c in contexts) / len(contexts) if contexts else 0
                },
                'stage4': kg_stats
            },
            'adaptive_weights': adaptive_weights,
            'pipeline': 'STAGE 1 (Ensemble) + STAGE 2 (Context) + STAGE 3 (Feedback) + STAGE 4 (Knowledge Graph)'
        })
        
    except Exception as e:
        print(f"[Complete Pipeline] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
