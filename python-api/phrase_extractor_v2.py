"""
Phrase Extractor V2 - ƒê√öNG CHU·∫®N H·ªåC THU·∫¨T
Tr√≠ch xu·∫•t PHRASES (kh√¥ng ph·∫£i words) t·ª´ sentences b·∫±ng TF-IDF n-gram
"""

import re
from typing import List, Dict, Tuple
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import nltk

# Download NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

ENGLISH_STOPWORDS = set(stopwords.words('english'))

# Vietnamese words to filter
VIETNAMESE_WORDS = {
    'yeu', 'nhan', 'lof', 'thcih', 'toi', 'ban', 'cho', 'cua', 'voi',
    'trong', 'ngoai', 'tren', 'duoi', 'giua', 'sau', 'truoc'
}


class PhraseExtractorV2:
    """
    Phrase Extractor V2 - Tr√≠ch xu·∫•t phrases t·ª´ sentences
    
    Pipeline:
    1. Split text ‚Üí sentences
    2. TF-IDF n-gram (2-3) tr√™n sentences
    3. Filter phrases (lo·∫°i stopwords, Vietnamese, errors)
    4. Return top phrases v·ªõi scores
    """
    
    def __init__(self):
        self.vectorizer = None
        self.sentences = []
        self.tfidf_matrix = None
    
    def extract_phrases(
        self,
        text: str,
        max_phrases: int = 50,
        min_phrase_length: int = 2,
        ngram_range: Tuple[int, int] = (2, 3)
    ) -> List[Dict]:
        """
        Extract phrases t·ª´ text
        
        Args:
            text: Input text
            max_phrases: S·ªë phrases t·ªëi ƒëa
            min_phrase_length: ƒê·ªô d√†i t·ªëi thi·ªÉu (s·ªë t·ª´)
            ngram_range: N-gram range (default: bigram + trigram)
        
        Returns:
            List of phrase dicts:
            [
                {
                    'phrase': 'soft skills',
                    'tfidf_score': 0.42,
                    'frequency': 3,
                    'sentences': [12, 15, 23]
                }
            ]
        """
        print(f"[PhraseExtractorV2] Starting extraction...")
        
        # B∆Ø·ªöC 1: Split th√†nh sentences
        self.sentences = self._split_sentences(text)
        
        if len(self.sentences) < 3:
            print(f"[PhraseExtractorV2] Not enough sentences ({len(self.sentences)})")
            return []
        
        print(f"[PhraseExtractorV2] Split into {len(self.sentences)} sentences")
        
        # B∆Ø·ªöC 2: TF-IDF n-gram tr√™n sentences
        phrases_with_scores = self._extract_tfidf_phrases(
            ngram_range=ngram_range,
            max_features=max_phrases * 3  # Extract more for filtering
        )
        
        print(f"[PhraseExtractorV2] Extracted {len(phrases_with_scores)} raw phrases")
        
        # B∆Ø·ªöC 3: Filter phrases
        filtered_phrases = self._filter_phrases(
            phrases_with_scores,
            min_phrase_length=min_phrase_length
        )
        
        print(f"[PhraseExtractorV2] After filtering: {len(filtered_phrases)} phrases")
        
        # B∆Ø·ªöC 4: Add metadata (frequency, sentence IDs)
        enriched_phrases = self._enrich_phrases(filtered_phrases, text)
        
        # B∆Ø·ªöC 5: Sort v√† return top phrases
        sorted_phrases = sorted(
            enriched_phrases,
            key=lambda x: x['tfidf_score'],
            reverse=True
        )[:max_phrases]
        
        print(f"[PhraseExtractorV2] Returning top {len(sorted_phrases)} phrases")
        
        return sorted_phrases
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        Split text th√†nh sentences
        """
        # Clean text tr∆∞·ªõc
        text = self._clean_text(text)
        
        # Tokenize sentences
        sentences = sent_tokenize(text)
        
        # Filter sentences qu√° ng·∫Øn
        valid_sentences = []
        for sent in sentences:
            words = word_tokenize(sent)
            if len(words) >= 5:  # √çt nh·∫•t 5 t·ª´
                valid_sentences.append(sent)
        
        return valid_sentences
    
    def _clean_text(self, text: str) -> str:
        """
        Clean text: lo·∫°i metadata, k√Ω t·ª± ƒë·∫∑c bi·ªát
        """
        # Remove URLs
        text = re.sub(r'http\S+|www\S+', '', text)
        
        # Remove email
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove multiple spaces
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def _extract_tfidf_phrases(
        self,
        ngram_range: Tuple[int, int],
        max_features: int
    ) -> List[Dict]:
        """
        Extract phrases b·∫±ng TF-IDF n-gram
        
        Returns:
            [{'phrase': 'soft skills', 'tfidf_score': 0.42}]
        """
        # TF-IDF vectorizer v·ªõi n-gram
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            min_df=2,                # Xu·∫•t hi·ªán √≠t nh·∫•t 2 sentences
            max_df=0.8,              # Kh√¥ng qu√° ph·ªï bi·∫øn
            stop_words='english',
            norm='l2',
            lowercase=True
        )
        
        try:
            # Fit tr√™n NHI·ªÄU sentences (kh√¥ng ph·∫£i 1 document!)
            self.tfidf_matrix = self.vectorizer.fit_transform(self.sentences)
            feature_names = self.vectorizer.get_feature_names_out()
            
            # Aggregate scores across all sentences
            mean_scores = self.tfidf_matrix.mean(axis=0).A1
            
            # Build phrase list
            phrases = []
            for idx, score in enumerate(mean_scores):
                if score > 0:
                    phrase = feature_names[idx]
                    phrases.append({
                        'phrase': phrase,
                        'tfidf_score': float(score)
                    })
            
            return phrases
            
        except Exception as e:
            print(f"[PhraseExtractorV2] TF-IDF error: {e}")
            return []
    
    def _filter_phrases(
        self,
        phrases: List[Dict],
        min_phrase_length: int
    ) -> List[Dict]:
        """
        Filter phrases:
        - Lo·∫°i stopwords ƒë∆°n
        - Lo·∫°i Vietnamese words
        - Lo·∫°i l·ªói ch√≠nh t·∫£
        - Lo·∫°i phrases qu√° ng·∫Øn
        """
        filtered = []
        
        for phrase_dict in phrases:
            phrase = phrase_dict['phrase']
            
            # Check 1: ƒê·ªô d√†i
            words = phrase.split()
            if len(words) < min_phrase_length:
                continue
            
            # Check 2: Kh√¥ng ph·∫£i to√†n stopwords
            if all(w in ENGLISH_STOPWORDS for w in words):
                continue
            
            # Check 3: Kh√¥ng ch·ª©a Vietnamese words
            if any(w in VIETNAMESE_WORDS for w in words):
                continue
            
            # Check 4: Ch·ªâ ch·ª©a k√Ω t·ª± ASCII
            if not all(ord(c) < 128 or c.isspace() for c in phrase):
                continue
            
            # Check 5: Kh√¥ng ph·∫£i to√†n s·ªë
            if phrase.replace(' ', '').replace('.', '').isdigit():
                continue
            
            # Check 6: √çt nh·∫•t 1 t·ª´ c√≥ nghƒ©a (kh√¥ng ph·∫£i stopword)
            meaningful_words = [w for w in words if w not in ENGLISH_STOPWORDS and len(w) >= 3]
            if len(meaningful_words) < 1:
                continue
            
            filtered.append(phrase_dict)
        
        return filtered
    
    def _enrich_phrases(self, phrases: List[Dict], text: str) -> List[Dict]:
        """
        Enrich phrases v·ªõi metadata:
        - Frequency (s·ªë l·∫ßn xu·∫•t hi·ªán)
        - Sentence IDs (xu·∫•t hi·ªán ·ªü sentences n√†o)
        """
        text_lower = text.lower()
        
        for phrase_dict in phrases:
            phrase = phrase_dict['phrase']
            
            # Count frequency
            frequency = text_lower.count(phrase)
            phrase_dict['frequency'] = frequency
            
            # Find sentence IDs
            sentence_ids = []
            for idx, sent in enumerate(self.sentences):
                if phrase in sent.lower():
                    sentence_ids.append(idx)
            
            phrase_dict['sentences'] = sentence_ids
            phrase_dict['n_sentences'] = len(sentence_ids)
        
        return phrases
    
    def get_phrase_context(self, phrase: str, max_contexts: int = 3) -> List[str]:
        """
        L·∫•y context sentences cho phrase
        
        Args:
            phrase: Phrase c·∫ßn t√¨m context
            max_contexts: S·ªë contexts t·ªëi ƒëa
        
        Returns:
            List of sentences ch·ª©a phrase
        """
        contexts = []
        
        for sent in self.sentences:
            if phrase in sent.lower():
                contexts.append(sent)
                if len(contexts) >= max_contexts:
                    break
        
        return contexts


# ============================================================================
# TESTING
# ============================================================================

if __name__ == "__main__":
    test_text = """
    Machine learning is a subset of artificial intelligence that enables computers 
    to learn from data. Deep learning uses neural networks with multiple layers.
    
    Natural language processing helps computers understand human language. 
    Computer vision allows machines to interpret visual information.
    
    Studying abroad helps students improve soft skills like teamwork and communication.
    Job opportunities in big companies require strong technical skills.
    
    Volunteer work provides valuable experience and helps develop leadership skills.
    Healthy lifestyle choices contribute to better mental and physical health.
    """
    
    print("=" * 80)
    print("TESTING PHRASE EXTRACTOR V2")
    print("=" * 80)
    
    extractor = PhraseExtractorV2()
    phrases = extractor.extract_phrases(
        text=test_text,
        max_phrases=10,
        ngram_range=(2, 3)
    )
    
    print("\nüìä RESULTS:")
    print("-" * 80)
    print(f"Total phrases extracted: {len(phrases)}\n")
    
    for i, phrase_dict in enumerate(phrases, 1):
        print(f"{i}. '{phrase_dict['phrase']}'")
        print(f"   TF-IDF: {phrase_dict['tfidf_score']:.4f}")
        print(f"   Frequency: {phrase_dict['frequency']}")
        print(f"   Appears in {phrase_dict['n_sentences']} sentences")
        
        # Show context
        contexts = extractor.get_phrase_context(phrase_dict['phrase'], max_contexts=1)
        if contexts:
            print(f"   Context: {contexts[0][:80]}...")
        print()
    
    print("‚úÖ Test completed!")
